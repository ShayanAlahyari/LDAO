\documentclass[12pt]{article}

% ------------------------------------------------
%                  PACKAGES
% ------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{subcaption}

% ------------------------------------------------
%           HYPERREF CONFIGURATION
% ------------------------------------------------
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue,
  pdfborder={0 0 0}
}

% -----------------------------
%  SINGLE-COLUMN PAGE SETUP
% -----------------------------
\usepackage[margin=1in]{geometry}

\begin{document}

\title{Weighted mixture distribution Oversampling for imbalanced regression}
\author{
    \textbf{xxx, xxx}
    \thanks{Manuscript received Month XX, 20XX.}
    % \thanks{This work was supported by ... (optional)}
    % \thanks{Corresponding author: ... (optional)}
}

\date{}
\maketitle

% ------------------------
%      ABSTRACT
% ------------------------
\begin{abstract}
Learning from imbalanced data is a major challenge in machine learning, especially for neural networks, which tend to perform poorly under such conditions. Imbalanced regression describes situations where the continuous target variable is distributed unevenly, resulting in sparse target regions and challenges comparable to those in imbalanced classification. Although class imbalance has been widely studied, the same problem in regression remains relatively underexplored and only a limited number of methods have been introduced. Existing resampling approaches often rely on arbitrary thresholding that splits the target range into rare vs normal categories, disregarding the underlying continuous distribution. This can yield synthetic samples that fail to mirror the actual continuous nature of the target, combining values that differ in behavior or importance and thereby impairing accurate regression. Additionally, other methods attempt to rebalance by undersampling the majority regions, which may improve balance at first glance but ultimately discards potentially valuable observations. This reduction in training data can degrade the model’s ability to learn nuanced patterns and, in turn, weakens overall predictive performance. Despite these efforts, there remains a gap in the literature for a method that effectively addresses the entire target distribution, improving both common outcomes and extreme tails while preserving important data. To address this gap, we propose WMDO (Mixture-Distribution Oversampling), a novel method for imbalanced regression that decomposes the target space into local sub-distributions, each preserving its distinct statistical characteristics, while maintaining consistency with the overall distribution, and rebalances each segment by modeling and oversampling from its local density. Unlike methods that only concentrate on sparse, tail regions, our approach addresses the entire target distribution, covering both high-density and low-density segments. This broader coverage improves performance on common values and rare extremes alike, whereas focusing solely on minority segments can undermine predictive accuracy for the rest of the target range. By oversampling each segment of the target space in isolation, our method preserves each segment’s unique features and generates synthetic data that genuinely reflects that segment’s distribution. As a result, the entire target space is more accurately represented, leading to better overall performance and improved predictions for rarely observed outcomes. Comprehensive evaluations on 43 datasets, including both low-dimensional and high-dimensional feature spaces, show that our method outperforms state-of-the-art oversampling approaches such as SMOGN, DenseLoss, and G-SMOTE. The code and data are available at:
\href{https://github.com/ShayanAlahyari/Distribution-Modeling-and-Ratio-Oversampling-for-Imbalanced-Regression}{https://github.com/ShayanAlahyari/BIKO}.
\end{abstract}

% ------------------------
%    INDEX TERMS
% ------------------------
\begin{flushleft}
\textbf{Index Terms}—Imbalanced regression, SMOTE, oversampling, ratio-driven sample generation, extreme value prediction, kernel-based sampling.
\end{flushleft}

% ------------------------
%     INTRODUCTION
% ------------------------
\section{Introduction}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{imbalanced_classification.png}
        \vspace{-1em}
        \caption{Two-class scatter plot with a decision boundary on synthetic imbalanced data.}
        \label{fig:imbalanced_classification}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{sparse_region.png}
        \vspace{-1em}
        \caption{Histogram of the target values from the boston dataset.}
        \label{fig:sparse_region}
    \end{subfigure}
    \caption{On the left is a classification example with a clearly labeled minority class that is easy to identify. On the right is an imbalanced regression example, where some target values appear in sparse regions throughout the distribution, making them harder to detect and handle.}
    \label{fig:two_figures_side_by_side}
\end{figure}

In classification tasks, imbalance occurs when certain classes have significantly fewer samples
compared to others, which can happen in both binary and multi-class settings
\cite{he2009}\cite{haixiang2017}. In these situations, the minority classes become overshadowed
by the majority classes, leading traditional classifiers to focus on the more prevalent classes
\cite{chawla2002}. This imbalance results in inaccurate detection of minority classes, as models
struggle to capture patterns within these sparsely sampled regions \cite{buda2018}.
Consequently, critical applications such as fraud detection, medical diagnostics, or fault
detection are especially affected, since small but important classes may be overlooked
\cite{johnson2019}. Accurate handling of imbalanced data is therefore essential for reliable
performance across all classes, ensuring that rare but significant instances receive proper
attention \cite{liu2009}.

While imbalance is commonly discussed in classification tasks \cite{he2009}, it also
occurs in regression tasks where the goal is to predict continuous numeric values
\cite{krawczyk2016}. Imbalanced regression specifically refers to situations where certain
ranges of target values — usually rare or extreme cases — have significantly fewer samples
than others \cite{branco2016,torgo2013}. In these scenarios, traditional regression models
often fail to accurately predict these rare target values because they are biased toward
more frequent, densely sampled regions \cite{chawla2004,branco2016}. This makes accurate
prediction in minority or scarce regions especially challenging, yet critically important
in many real-world applications \cite{torgo2013}. Figure~\ref{fig:two_figures_side_by_side} compares imbalanced classification with imbalanced regression. In subfigure~\ref{fig:imbalanced_classification}, identifying a minority class is straightforward because each label is discrete; one can simply count how many samples fall under each label. In subfigure~\ref{fig:sparse_region}, however, a minority region appears within the continuous target distribution itself, where some outcomes are found in sparsely populated tails. Unlike classification, where imbalance is tied to distinct labels, regression imbalance is defined by the distribution’s shape (which can be skewed or multi-modal). Consequently, standard approaches from classification do not readily extend to regression, as there is no simple way to “count labels” to isolate underrepresented target values. This makes accurately detecting and mitigating sparse ranges a key challenge in imbalanced regression.

 


Many real-world applications are struggling with imbalanced data. In genomic prediction, continuous trait values such as pathogenicity scores are frequently skew toward certain levels, leaving extreme, deleterious variants in sparsely populated tails. Standard regression models often struggle with these infrequent but clinically critical extremes\cite{kaur24}. In spatio-temporal forecasting of extreme wind events, most observations lie around moderate wind speeds, while very high speed wind events, crucial for hazard warnings, energy grid stability, and wind farm operations, occur only rarely. As a result, conventional regression models trained on the bulk of moderate data tend to underestimate or completely miss these high impact outliers, leading to compromised performance precisely when accurate forecasts are most essential\cite{scheepens23}. In healthcare, hospital length‐of‐stay data often skews heavily. Most patients are discharged quickly, but a smaller subset remains hospitalized much longer. If models only learn from short‐stay cases, they fail to capture the extended‐stay minority leading to underestimates of these resource‐intensive admissions. Accurate modeling of rare, longer stays is crucial for managing bed capacity and staff scheduling, particularly under high‐stress conditions such as a pandemic\cite{xu2022}.


In response, specialized techniques have emerged. Some adapt SMOTE by creating synthetic instances in the tail ends of the target distribution\cite{torgo2013}\cite{branco2017}. Some rely on cost-sensitive frameworks that penalize large errors on rare events\cite{steininger2021}.

Despite the complexity of this issue, genuinely comprehensive approaches remain quite scarce, with far fewer methodologies developed for imbalanced regression than have been explored for imbalanced classification.

 In this article, we address the problem of imbalanced regression. Our contributions are as follows:
\begin{itemize}
    \item We introduce Mixture-Distribution Oversampling (WMDO), a new approach designed explicitly for imbalanced regression.
    \item WMDO accounts for all conditional distributions across the entire continuous target space, not only underrepresented regions, by independently generating synthetic examples that preserve the overall distribution while still focusing on tail or sparse segments.



\end{itemize}

This article has the following sections. Section~\ref{sec:problem} defines the imbalanced 
regression problem, Section~\ref{sec:related} reviews related work, and 
Section~\ref{sec:conclusions} presents the conclusions.


% ------------------------
%     PROBLEM DEFINTION
% ------------------------
\section{Problem definition}
\label{sec:problem}







% ------------------------
%     RELATED WORK
% ------------------------
\section{Related Work}
\label{sec:related}

Data-Level Approaches (Resampling): Early solutions for imbalanced data focused on data-level methods, especially in classification. These techniques rebalance the class distribution by generating new minority instances or removing majority instances\cite{chawla2002}\cite{he2008}. For example, SMOTE introduced synthetic minority examples by interpolating between neighbors\cite{chawla2002}, and it inspired many variants such as Borderline-SMOTE, which focuses on difficult boundary samples\cite{han2005}, and density-based methods like DBSMOTE\cite{bunkhumpornpat2012}.

Oversampling methods remain popular nearly one-third of imbalanced classification papers use data-level strategies\cite{haixiang2017}, but most were designed for discrete classes. In regression, adapting these ideas is less straightforward due to a continuous target space\cite{he2013}\cite{krawczyk2016}. The first imbalanced regression methods extended classification resampling by defining a relevance function to identify “rare” vs. “common” target regions\cite{torgo2007}. This relevance function $\phi(y)$ assigns higher scores to extreme (rare) target values, allowing a user-defined threshold to split data into rare and normal subsets\cite{torgo2007}\cite{ribeiro2011a}. Using this framework, Torgo et al. proposed two seminal data-level approaches for regression: a random undersampling that discards low-$\phi(y)$ (common) instances, and SMOTER, an oversampling technique creating new samples in high-$\phi(y)$ regions by interpolating feature values and assigning a target value via weighted average of neighbors\cite{torgo2013}\cite{torgo2015}. These methods were among the first to tackle regression imbalance and showed that rebalancing can shift model focus to rare targets\cite{torgo2015}. However, like their classification counterparts, they rely on a user-chosen rarity threshold that may be arbitrary\cite{branco2019}.

Subsequent data-level innovations for regression built on SMOTER’s foundation. SMOGN (Synthetic Minority Over-sampling Technique for Regression with Gaussian Noise) improved SMOTER by combining interpolation with noise-based oversampling. For each rare instance, either an interpolated synthetic point (as in SMOTER) or a perturbed point with added Gaussian noise is generated\cite{branco2017}. This hybrid approach oversamples sparse regions more flexibly and was reported to outperform the SMOTER\cite{branco2017}. SMOGN also undersamples the abundant “normal” examples, and by integrating these steps, it became regarded as a state-of-the-art resampling method for imbalanced\cite{branco2017}. Beyond SMOGN, other oversampling tactics have been explored. \cite{branco2019} introduced simple random oversampling (replicating high-$\phi$ examples) and a pure noise-based oversampling (adding small random perturbations to rare examples) for regression tasks. They also proposed WERCS (WEighted Relevance-based Combination Strategy), which probabilistically chooses to oversample or undersample each instance based on its relevance score: rare instances have a higher chance to be duplicated (and still a small chance to be removed), while common instances are usually undersampled unless occasionally kept for diversity. This strategy blends oversampling and undersampling in one unified procedure, aiming to avoid a hard threshold cut-off\cite{branco2019}. 

More recently, \cite{camacho2022} have looked at geometric variations of SMOTE for regression. Geometric SMOTE for regression (G-SMOTE) adapts a geometrically enhanced SMOTE to continuous targets\cite{camacho2022}. G-SMOTE for regression generates synthetic samples in minority regions by considering not only feature-space interpolation but also the geometry of target distributions, offering greater flexibility in how new examples are placed\cite{camacho2022}.
Overall, data-level methods in regression from SMOTER and SMOGN to WERCS and G-SMOTE share the common goal of populating scarce target ranges with additional examples. They have demonstrated improved prediction of extreme values but they often depend on defining rarity thresholds or nearest-neighbor parameters that may not generalize across datasets\cite{branco2019}.

Algorithm-Level Approaches (Cost-Sensitive Methods): Instead of modifying the training data, algorithm-level approaches embed imbalance handling into the learning process. Cost-sensitive learning is a key strategy: the idea is to adjust the loss function or instance weights so that errors on rare targets incur higher cost\cite{zhou2010}. this concept has long been used in classification, e.g. weighting classes inversely to their frequency\cite{elkan2001}\cite{domingos1999}, and it naturally extends to regression by allowing the model to pay more attention to extreme values.

DenseLoss is one such method specifically designed for imbalanced regression\cite{steininger2021}. It employs a density-based weighting scheme (DenseWeight), where each training sample is weighted according to the rarity of its target value\cite{steininger2021}. In practice, DenseWeight estimates the probability density of the target distribution (via kernel density estimation) and assigns larger weights to low-density (rare) targets. These weights scale the loss function during training, effectively emphasizing prediction accuracy on rare cases. Unlike earlier oversampling methods, DenseLoss does not create or remove examples. Yang et al. \cite{yang2021} propose a framework for deep imbalanced regression, focusing on improving the prediction performance for underrepresented (extreme) target values. Their method includes a label-distribution smoothing technique and adaptive sampling, highlighting how imbalance in continuous targets can severely degrade performance in deep neural networks. Ren et al. \cite{ren2022} introduce Balanced MSE, which modifies the mean-squared error so that errors on rare targets receive higher weight; the authors validate it on multiple computer vision tasks, demonstrating that directly addressing label imbalance outperforms standard MSE in skewed regression settings. Moniz et al. \cite{moniz2018}, on the other hand, extend boosting to handle rare target values using SMOTEBoost for regression: they generate synthetic points for the minority (extreme) ranges of the target, then apply a boosting algorithm to emphasize these synthetic examples. This combined over-sampling and ensemble strategy is especially useful for predicting outliers or tail values where standard regressors might fail. In imbalanced regression specifically, cost-sensitive schemes like DenseLoss are still relatively few but show promise by avoiding the need to augment data synthetically\cite{steininger2021}. A potential drawback, however, is the sensitivity of training to the chosen weighting function – if rare cases are over-weighted, optimization can become unstable\cite{he2009}.

Ensemble Methods and Evaluation Metrics: Another line of research considers ensemble learning to tackle imbalanced regression. Ensemble methods like boosting and bagging inherently improve generalization by combining multiple learners, but on their own they do not address skewed distributions\cite{hoens2013}. Researchers have therefore applied ensemble techniques in conjunction with the above strategies. For example, a common approach is to incorporate sampling into each round of an ensemble – Resampled Bagging trains each bootstrap replicate on a balanced subset or weighted instances, leveraging diversity while mitigating bias toward the majority targets\cite{branco2019}.

Finally, evaluation for imbalanced regression has evolved alongside modeling techniques. Traditional metrics like MSE or MAE can be dominated by errors on frequent cases, so specialized metrics have been proposed to fairly assess performance on rare targets. The SERA metric (Smooth Error Rate Approximation) is one such example – it integrates the concept of relevance into evaluation, effectively weighting errors by the relevance (rareness) of the target value\cite{ribeiro2020}. \cite{ribeiro2020} argue that SERA provides a more informative comparison of models in imbalanced domains, as it highlights the models’ ability to predict extreme values versus common values. Although SERA is an evaluation tool (not a training algorithm), it reflects the community’s growing emphasis on capturing rare-event performance.



% ------------------------
%         MOTIVATION
% ------------------------
\section{Motivation}
\label{sec:mot}



A common thread among these methods is the challenge of preserving the true continuous distribution of the target. Most oversampling approaches (SMOTER, SMOGN, G-SMOTE-R) rely on identifying minority regions via some threshold or relevance scoring, effectively simplifying a continuous spectrum into categories. This can be arbitrary and problem-dependent. Additionally, methods that generate synthetic data through interpolation or noise might distort the original data distribution, especially if rare samples are very sparse. While algorithm-level solutions like DenseLoss avoid data distortion, they don’t oversample the dataset with new examples. These limitations set the stage for more advanced approaches aiming to better respect the continuous nature of regression targets.

Despite the progress made by the above techniques, several gaps remain in addressing all aspects of imbalanced regression:

\begin{itemize}
    \item Arbitrary Binarization of Continuity: Many methods fundamentally convert a continuous target problem into a pseudo-classification one by splitting the target range into “rare” and “common” bins. This reliance on a hard threshold or discrete binning is somewhat arbitrary – a slightly different cutoff could change which points are considered rare. Such binarization can fail to capture gradations in the rarity of values (e.g. moderately rare vs extremely rare) and may ignore useful information about the target distribution’s shape.
    \item Preservation of Target Distribution: A core challenge is generating or re-weighting data in a way that preserves the underlying distribution of the target values. Oversampling via interpolation (as in SMOTER/SMOGN) or adding noise can alter the marginal distribution of $y$ – for instance, by over-populating certain interpolation ranges or introducing values that slightly deviate from the original data trends. If the minority examples are very sparse, synthetic samples might not accurately reflect true data patterns, effectively introducing noise or bias. Some methods risk creating synthetic targets that do not follow the real distribution (e.g., interpolation could produce a target value that is an average of two rare cases, which might be less “extreme” than the originals). The literature indicates that generating a new $y$ for synthetic cases is non-trivial – one must ensure the synthetic features-target pair is plausible. In short, existing oversampling techniques struggle to capture complex or multimodal distributions of continuous targets, often focusing only on the extreme tails and not the overall distribution shape.
    \item Reliance on Expert or Data-Specific Tuning: Because methods like SMOTER/SMOGN depend on a relevance function and threshold, they often require domain knowledge or trial-and-error to set these parameters. What counts as “rare” in one dataset may not be in another. If the relevance function is mis-specified or the threshold poorly chosen, oversampling might either overshoot (creating too many synthetic points in not-so-rare regions) or undershoot (failing to cover truly rare values). Thus, the lack of a robust, distribution-agnostic oversampling criterion remains an issue.

Many real-world applications in healthcare, genomics, environmental science, finance, and engineering involve regression tasks on highly imbalanced datasets. Existing data-level solutions for such tasks are limited, and the few that do exist frequently produce synthetic examples that fail to preserve the true underlying data distribution. Simply undersampling well-represented regions and oversampling sparse regions may improve predictions for rare targets but often degrades performance on more frequent targets. Moreover, complex or multimodal distributions cannot be adequately captured by threshold-based methods or simple interpolation.

In contrast, the proposed WMDO approach focuses on preserving the nuanced structure of the data across all target ranges. Rather than discarding common samples or relying solely on interpolation in sparse regions, WMDO oversamples each segment of the target distribution by creating synthetic data that respects the conditional distribution of both rare and frequent values. By treating each range independently, without forcing a global threshold, we retain information from the dense parts of the distribution while properly augmenting the sparse ones. When these oversampled segments are recombined into the overall dataset, the result is a smooth, unified distribution that improves predictive performance for both common and extreme targets.

\end{itemize}




% ------------------------
%         METHOD
% ------------------------
\section{The proposed method}
\label{sec:pmethod}


We introduce Weighted Mixture Distribution Oversampling (WMDO), a novel method that views the target variable not as one monolithic distribution but rather as multiple mixture components, each covering a distinct range of values. Unlike existing oversampling methods such as SMOTER, SMOGN, and G-SMOTE, which rely on predefined relevance functions, thresholding rules, or heuristic rarity labeling to identify underrepresented target regions, WMDO automatically determines its segmentation based on the natural distribution of the data. SMOTER and SMOGN require manually setting a relevance function to classify rare and normal values, introducing dataset-specific tuning and making the results sensitive to threshold selection. Similarly, G-SMOTE applies geometric oversampling but still requires a pre-classification step to determine minority target ranges before generating synthetic points. In contrast, WMDO eliminates the need for any predefined rarity criteria. This approach ensures a fully adaptive and data-driven segmentation, effectively capturing both dense and sparse regions without relying on arbitrary user-defined thresholds or expert domain knowledge.


By analyzing and rebalancing each mixture component separately, WMDO ensures accurate modeling of both rare, extreme targets and more common, central ranges. This local focus prevents the distortions and arbitrary cutoffs seen in threshold-based methods, allowing the technique to capture subtle nuances within each sub-distribution. As a result, WMDO not only boosts predictions for rare outcomes (often critical in real-world applications) but also maintains strong performance for typical values. Taken together, these features produce a comprehensive, balanced solution that surpasses existing oversampling strategies for imbalanced regression.


\begin{figure}[!t]
    \vspace{-1cm}   % Adjust to suit
    \centering
    \includegraphics[width=\textwidth,
    % height=0.4\textheight,
    ]{Your paragraph text (10).png}
    \caption{WMDO oversamples each conditional distribution of the target  independently, resulting in a balanced final distribution.}
    \label{fig:method_large_image}
\end{figure}

\noindent
\textbf{5.2 Estimating the Target Distribution}

\noindent
A core part of our proposed oversampling method is automatically choosing a parametric distribution that approximates the observed target values. Many real-world datasets exhibit skewness, heavy tails, or multi-modal behavior. Hence, assuming that the target must be, for example, normally distributed, can lead to a poor fit. By systematically comparing multiple candidate distributions, we avoid imposing an overly restrictive shape on the data. This procedure is particularly important in \textit{imbalanced regression}, where accurately modeling tails and extreme values is critical.

\vspace{6pt}
\noindent
\textbf{5.2.1 Why We Need a Parametric Approximation}

\noindent
Existing methods often rely on hard-coded thresholds or binning heuristics to identify ``rare'' target values. However, such heuristics can be arbitrary and may not capture the nuanced structure of the data. In contrast, fitting a parametric distribution directly to the observed targets:

\begin{itemize}
  \item Flexibly captures the actual shape (skew, tails, or multiple modes), thus avoiding one-size-fits-all assumptions.
  \item Provides a systematic basis for segmenting the continuous range into sub-distributions, letting the data guide where these segments should lie.
  \item Preserves continuity of the target variable, reducing the risk of over- or under-weighting important sparse regions.
\end{itemize}

\vspace{6pt}
\noindent
\textbf{5.2.2 Candidate Distributions and Fitting}

\noindent
Let $\{y_1, y_2, \dots, y_N\}$ represent the observed target values. Suppose we have a collection of $K$ candidate distributions, $\mathcal{D}_1, \mathcal{D}_2, \dots, \mathcal{D}_K$ (for instance, Normal, Gamma, Weibull, etc.). Each distribution $\mathcal{D}_k$ is characterized by a parameter vector $\theta_k$ and has:

\begin{itemize}
  \item \textbf{Probability Density Function (PDF):} 
  \[
    p_k(y \mid \theta_k)
  \]
  \item \textbf{Cumulative Distribution Function (CDF):} 
  \[
    F_k(y \mid \theta_k)
  \]
\end{itemize}

\noindent
To fit $\theta_k$ for each candidate, we use Maximum Likelihood Estimation (MLE) on our sample $\{y_i\}$. Specifically, we solve:

\[
\hat{\theta}_k 
= \arg \max_{\theta} 
\prod_{i=1}^{N} p_k(y_i \mid \theta).
\]

\noindent
In practice, we often optimize the \textit{log-likelihood}, which is numerically more stable:

\[
\hat{\theta}_k 
= \arg \max_{\theta}
\sum_{i=1}^{N} \ln \bigl[p_k(y_i \mid \theta)\bigr].
\]

\noindent
This step can be performed using standard numerical methods (for example, gradient-based algorithms or dedicated routines in statistical libraries), returning the parameter values that best fit the data under each candidate distribution.

\vspace{6pt}
\noindent
\textbf{5.2.3 Goodness of Fit via Kolmogorov-Smirnov}

\noindent
Once we have $\hat{\theta}_k$ for each distribution, we compare the resulting fitted model against the \textit{empirical} distribution of the data. Let $F_n(y)$ be the empirical CDF, defined by:

\[
F_n(y) = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}\{y_i \le y\},
\]

\noindent
where $\mathbf{1}\{\cdot\}$ is the indicator function. The Kolmogorov-Smirnov (K-S) statistic measures the maximum vertical gap between $F_n(y)$ and $F_k\bigl(y \mid \hat{\theta}_k\bigr)$:

\[
D_k 
= \sup_{y \in \mathbb{R}} 
\Bigl| F_n(y) - F_k\bigl(y \mid \hat{\theta}_k\bigr)\Bigr|.
\]

\noindent
A smaller $D_k$ indicates that $\mathcal{D}_k$ matches the empirical distribution more closely across the entire range of $y$. Thus, distributions with poor tail coverage or large deviations in any region of $y$ will generally exhibit a higher $D_k$.

\vspace{6pt}
\noindent
\textbf{5.2.4 Selecting the Best Distribution}

\noindent
We pick the distribution whose fitted CDF yields the smallest K-S distance:

\[
k_{\ast} =
\arg\min_{1 \le k \le K} D_k,
\quad\text{and then set}\quad
(\mathcal{D}_{\ast}, \hat{\theta}_{\ast})
=
(\mathcal{D}_{k_{\ast}}, \hat{\theta}_{k_{\ast}}).
\]

\noindent
Since the K-S test highlights maximum deviation rather than an average discrepancy, it is more sensitive to skewness or long tails. Consequently, using K-S is especially helpful for imbalanced regression, where performance on rare or extreme targets is critical.

\vspace{6pt}
\noindent
\textbf{5.2.5 Practical Outcome}

\noindent
Upon completion of this procedure, we have:

\begin{itemize}
  \item \textbf{A chosen distribution} $\mathcal{D}_{\ast}$ that best aligns with the data.
  \item \textbf{A corresponding parameter set} $\hat{\theta}_{\ast}$ fully specifying its shape and scale.
  \item \textbf{A data-driven, smooth approximation} of the target distribution, free from ad hoc thresholds or bins.
\end{itemize}

\noindent
This fitted distribution then guides our intervals of the continuous target range into sub-distributions, naturally reflecting both common and sparse regions. As a result, the subsequent oversampling strategy can precisely oversample areas with few samples without distorting or discarding well-populated segments. By capturing the true underlying shape, we build a robust foundation for rebalancing the target space in an imbalanced regression setting.

% -------------------------------------
% 5.3 Determining the Number of Intervals via Minimum Sample Count
% -------------------------------------
\noindent
\textbf{5.3 Determining the Number of Intervals via Minimum Sample Count}

\noindent
When segmenting the target space into subdistributions, a practical consideration is how many intervals to create. Too few intervals may group together distinctly different regions of the target distribution, while too many intervals may lead to extremely sparse subranges that cannot be effectively oversampled. In WMDO, one approach is to impose a minimum sample count per interval, ensuring each subrange of the target distribution receives enough coverage from real data before synthetic points are introduced.

\medskip
\noindent
\textbf{5.3.1 Motivation}

\noindent
\textit{Avoid Excessive Sparsity:} An interval containing only a handful of points can yield unreliable estimates of local density. Imposing a minimum count ensures that each interval has sufficient “core” data.

\noindent
\textit{Adaptive to Varying Dataset Sizes:} Larger datasets naturally permit more intervals (more finely grained subranges), while smaller datasets require fewer intervals to keep each segment well populated.

\noindent
\textit{Simplicity and Interpretability:} Unlike methods that rely on intricate heuristics or domain‐dependent thresholds, the minimum‐sample‐count rule provides a straightforward, integer‐based criterion that is easy to tune across different tasks.

\medskip
\noindent
\textbf{5.3.2 Mathematical Formulation}

\noindent
Let \(\{y_{1}, y_{2}, \dots, y_{N}\}\) be the observed target values in the training set. Suppose we wish to ensure that each interval contains at least \(\alpha\) points on average. One can pick the number of intervals \(M\) by:
\[
  M \;=\; \left\lfloor \frac{N}{\alpha} \right\rfloor,
\]
where
\begin{itemize}
  \item \(N\) is the total number of data points in the training set,
  \item \(\alpha\) is the minimum sample count we target per interval,
  \item \(\lfloor \cdot \rfloor\) denotes the floor function.
\end{itemize}
In practice, some intervals may have slightly more or fewer than \(\alpha\) points, because we later split the feasible range of \(y\) according to the fitted distribution’s quantiles (see Section 5.2). However, on average, each interval will meet the desired minimum density.

Once \(M\) is determined, we partition the range of \(y\) into \(M\) intervals by evaluating 
the inverse CDF (the percentile point function, \(\mathrm{PPF}\)) of the selected distribution 
\(D_{\ast}\) (with parameters \(\hat{\theta}_{\ast}\)) at a uniform grid of conditional probabilities:
\[
  q_{i} \;=\; \frac{i}{M}, 
  \quad i = 0, 1, \dots, M.
\]
\emph{Denote the lower and upper bound of the \(i\)-th interval by}
\[
  y_{L}^{(i)} \;=\; \mathrm{PPF}\bigl(q_{\,i-1}\bigr),
  \quad
  y_{U}^{(i)} \;=\; \mathrm{PPF}\bigl(q_{\,i}\bigr).
\]
Accordingly, each \(\mathrm{Interval}_{\,i}\) is defined as
\[
  \mathrm{Interval}_{\,i} 
  \;=\; 
  \bigl\{\,y : 
    y_{L}^{(i)} \;\le\; y \;<\; y_{U}^{(i)}
  \bigr\},
  \quad i = 1, \dots, M.
\]
Because \(M \approx \tfrac{N}{\alpha}\), we expect, on average, around \(\alpha\) data points 
per interval. In practice, some intervals may have slightly more or fewer than \(\alpha\) 
points, because we later split the feasible range of \(y\) according to the fitted 
distribution’s quantiles (see Section~5.2). However, this approach maintains a roughly 
uniform allocation of samples across the target space.


\medskip
\noindent
\textbf{5.3.3 Practical Implementation}

\noindent
\textit{Choosing \(\alpha\):} Typical values for \(\alpha\) may range from 10 to 50, depending on how large \(N\) is. A smaller \(\alpha\) yields more, finer intervals; a larger \(\alpha\) creates fewer, broader intervals.

\noindent
\textit{Handling Edge Cases:}
\begin{itemize}
    \item If \(N < \alpha\), then \(\bigl\lfloor \tfrac{N}{\alpha} \bigr\rfloor\) becomes 0, so we default to \(M = 1\) (a single interval spanning the entire range).
    \item If the fitted distribution’s \(\mathrm{PPF}\) fails for certain probabilities (for example, due to very heavy tails), numerical checks or fallback procedures (such as capping at the min or max observed values) may be used.
\end{itemize}

\subsubsection{Why Use the PPF (Percentile Point Function)?}
\label{sec:why_ppf}

When defining local conditional sub‐distributions for the continuous target variable,
one must ensure each subrange has adequate data coverage.
A naive approach would be to place uniform intervals over
\(\bigl[y_{\min}, y_{\max}\bigr]\) (e.g., \(\bigl[y_{\min},\,y_{\min} + \Delta\bigr)\),
\(\bigl[y_{\min} + \Delta,\,y_{\min} + 2\Delta\bigr)\), etc.). 
This can fail for skewed or multi‐modal targets: tail subranges typically end up
with few points, whereas high‐density regions become overcrowded.

\medskip
\noindent
WMDO instead adopts a \emph{probability‐based} approach to defining sub‐distributions.
Let \(F_{\ast}(y)\) be the cumulative distribution function (CDF) of the chosen 
parametric model \(\mathcal{D}_{\ast}\) (Section~5.2), and 
\(\mathrm{PPF}_{\ast}(q) = F_{\ast}^{-1}(q)\) its percentile point function.
For an integer \(M\), we pick uniform probabilities 
\(q_0 = 0,\; q_1 = \tfrac{1}{M}, \dots, q_M = 1,\)
and then let each conditional sub‐distribution be determined by:
\[
  \bigl[\mathrm{PPF}_{\ast}(q_{i-1}),\,\mathrm{PPF}_{\ast}(q_{i})\bigr),
  \quad i=1,\dots,M.
\]
Since every subrange corresponds to \(\tfrac{1}{M}\) of the total distribution’s mass,
the widths of these subranges adapt automatically to different densities or tails.

\medskip
\noindent
\textbf{Key Benefits of PPF‐Based Sub‐Distribution Definition.}
\begin{itemize}
    \item \emph{Balanced Support:} If \(\mathcal{D}_{\ast}\) approximates the target distribution
    well, each subrange contains a comparable fraction of samples, reducing the likelihood of
    sparse or overloaded regions.

    \item \emph{Adaptation to Skew/Tails:} By dividing \emph{probability} space uniformly,
    WMDO prevents extremely small subranges in heavy‐tailed areas and avoids unduly
    narrow bins in dense areas.

    \item \emph{Alignment with the Fitted Model:} Defining sub‐distributions via
    \(\mathrm{PPF}_{\ast}\) seamlessly incorporates the shape parameters derived in
    Section~5.2, rather than relying on an arbitrary or uniform numeric grid.
\end{itemize}

\noindent
Overall, this probability‐oriented segmentation works in tandem with the 
minimum‐sample‐count rule (Section~5.3.2),
providing both a target number of data points per subrange 
\emph{and} adaptive boundaries that capture the natural structure 
of the underlying distribution.




\bigskip
% -------------------------------------
% 5.4 Freedman-Diaconis Rule as an Alternative
% -------------------------------------
\noindent
\textbf{5.4 Freedman-Diaconis Rule as an Alternative}

\noindent
While the minimum‐sample‐count method is straightforward and effective, other data‐driven rules can also determine interval widths or interval counts for segmenting the target distribution. One such notable option is the Freedman-Diaconis rule, which automatically calculates an optimal interval width based on the data’s interquartile range (IQR) and sample size.

\medskip
\noindent
\textbf{5.4.1 Mathematical Derivation}

\noindent
Let again \(\{y_{1}, y_{2}, \dots, y_{N}\}\) be our target values, sorted in ascending order. The Freedman-Diaconis rule proposes:
\[
\text{IntervalWidth} \;=\; \frac{2\,\mathrm{IQR}(y)}{N^{1/3}}.
\]
where 
\[
  \mathrm{IQR}(y) 
  \;=\; 
  Q_{3}(y) - Q_{1}(y),
\]
with \(Q_{1}(y)\) and \(Q_{3}(y)\) being the first and third quartiles of the observed data. This interval width attempts to balance two competing objectives:
\begin{itemize}
    \item \textit{Data Smoothness:} Increasing the interval width lumps more data points together, reducing noise in each interval.
    \item \textit{Local Resolution:} Decreasing the interval width provides more granular coverage, especially around peaks or tails.
\end{itemize}
Once we fix \(\text{IntervalWidth}\), the total number of intervals \(M_{\mathrm{FD}}\) for a dataset in the range \([\min(y), \max(y)]\) is approximately:
\[
  M_{\mathrm{FD}}
  \;=\;
  \left\lceil \frac{\max(y) - \min(y)}{\text{IntervalWidth}} \right\rceil.
\]
Because the Freedman-Diaconis rule directly reflects the underlying dispersion (via IQR) and the sample size \(N\), it is often a robust default for histogramming in univariate data analysis.

\medskip
\noindent
\textbf{5.4.2 Practical Considerations}

\noindent
\textit{Tails and Skewness:}
The Freedman-Diaconis rule can produce wide intervals for heavily skewed distributions (due to a large IQR), or very narrow intervals if the data is highly concentrated. This characteristic can be beneficial, but practitioners must watch for extreme outliers that inflate the IQR.

\noindent
\textbf{Integration with Fitted Distribution:}
Freedman-Diaconis interval boundaries can be intersected with the parametric distribution \(D_{\ast}\) from Section~5.2 to refine subrange endpoints. Alternatively, Freedman-Diaconis may be applied directly to the empirical range \([\min(y), \max(y)]\) without referencing \(D_{\ast}\). This choice depends on whether the intervals should align explicitly with the fitted model or follow a purely data-driven approach.


\noindent
\textit{Comparison to Minimum Sample Count:}
\begin{itemize}
    \item \textit{Freedman-Diaconis:} Basing intervals on the local spread of data (via IQR) and \(N\).
    \item \textit{Minimum Sample Count:} Ensuring each interval has a sufficient number of real data points for stable local oversampling.
\end{itemize}
Both approaches can be valid, and often the choice depends on domain context or empirical performance. In some datasets, Freedman-Diaconis yields nicely balanced intervals, while in others the minimum‐sample‐count rule may ensure better coverage in critical tail regions.

\noindent
Overall, Freedman-Diaconis provides a well‐established, principled way to determine interval widths, requiring no additional hyperparameters. On the other hand, the minimum‐sample‐count method offers direct control over how many data points end up in each interval, making it simpler to guarantee robust local density estimates in extremely imbalanced or heavy‐tailed datasets.


\begin{figure}[!t]
    \vspace{-1cm}   % Adjust to suit
    \centering
    \includegraphics[width=\textwidth,
    % height=0.4\textheight,
    ]{pic12.png}
    \caption{WMDO decomposes the global target distribution into multiple sub-distributions (Sections XXX and YYY). Each sub-distribution is then modeled with its own KDE, from which samples are independently drawn.}
    \label{fig:method_large_image}
\end{figure}


\subsection{Joint Kernel Density Estimation for Conditional Oversampling}
\label{sec:kde_joint_oversampling}

Having split the target space into intervals via the fitted distribution’s PPF
(Section~\ref{sec:why_ppf}), each subrange $[y_{L},\, y_{U}]$ now contains only those training
examples $(\mathbf{x}_i, y_i)$ whose target $y_i \in [y_{L}, y_{U}]$. Our goal is to
\emph{oversample} this subrange by generating additional synthetic pairs $(\mathbf{x}, y)$ that
respect the local structure of both the features $\mathbf{x} \in \mathbb{R}^{d}$ and the target
$y \in \mathbb{R}$. Specifically, we estimate the \emph{joint} density 
$p(\mathbf{x}, y)$ within that subrange and sample from it.

\paragraph{Rationale.} Unlike simple interpolation methods that generate new $y$ values by
averaging or perturbing existing targets (e.g.\ SMOTER/SMOGN), a joint density estimation
explicitly models how $\mathbf{x}$ and $y$ co-vary. This leads to more realistic
$(\mathbf{x}, y)$ pairs since every synthetic example is drawn from a smooth approximation
of the local data manifold in \emph{both} feature space and target space. As a result, we
better preserve subtle correlations that might exist in each subrange.

\subsubsection{Kernel Density Estimation in $\mathbf{R}^{d+1}$}

Let $\mathcal{S} = \{\bigl(\mathbf{x}_i, y_i\bigr)\}_{i=1}^n$ be the subset of training points
falling into one subrange, where $n = |\mathcal{S}|$ denotes the \emph{local} subrange size
(distinct from $N$, the total number of training samples).
In total, $\mathbf{x}_i \in \mathbb{R}^{d}$ and $y_i \in \mathbb{R}$, so
each data point lives in $\mathbb{R}^{d+1}$. We seek a smooth estimate of the unknown
joint distribution $p(\mathbf{x}, y)$ from which $\mathcal{S}$ was drawn.


\paragraph{Definition (KDE).}
A \emph{kernel density estimator} for $p(\mathbf{x}, y)$ given $\mathcal{S}$ is:
\begin{equation}
    \label{eq:kde}
    \widehat{p}_{h}(\mathbf{x}, y)
    \;=\;
    \frac{1}{n}
    \sum_{i=1}^n 
    \; \frac{1}{\sqrt{\det(\mathbf{H})}}\,
    K\!\Bigl(\mathbf{H}^{-1/2}\bigl[(\mathbf{x}, y) - (\mathbf{x}_i, y_i)\bigr]\Bigr),
\end{equation}
where:
\begin{itemize}
    \item $K(\cdot)$ is a \textbf{kernel function}, typically chosen to be a Gaussian:
    \[
       K(\mathbf{z})
       \;=\;
       \frac{1}{(2\pi)^{\tfrac{d+1}{2}}}
       \exp\!\Bigl(-\tfrac{1}{2}\|\mathbf{z}\|^2\Bigr).
    \]
    \item $\mathbf{H}$ is the \textbf{bandwidth matrix}, a positive-definite $(d+1)\times(d+1)$
    matrix that controls the smoothing. In practice, many implementations simplify $\mathbf{H}$
    to $h^2\,\mathbf{I}$, where $h>0$ is a scalar bandwidth parameter.
\end{itemize}

\paragraph{Choice of Bandwidth $h$.}
Selecting an appropriate $h$ is crucial. A bandwidth too large over-smooths the
estimator, ``blurring out'' important local structures; a bandwidth too small
overfits by creating spikes around each data point. In our algorithmic framework
(Section~\ref{sec:pmethod}), we \emph{learn} $h$ (and possibly other hyperparameters) via
a validation procedure---balancing the trade-off between smoothness and data fidelity. 
This automatic tuning ensures robust adaptation across different subranges and datasets.

\subsubsection{Sampling Synthetic Points from the KDE}

After fitting the kernel density estimator \eqref{eq:kde} to the subrange $\mathcal{S}$, we
wish to \textbf{generate} $N_{\mathrm{new}}$ synthetic samples $(\tilde{\mathbf{x}}, \tilde{y})$
that follow $\widehat{p}_{h}(\mathbf{x}, y)$. One common way to draw samples is to exploit
the mixture representation:
\begin{equation}
    \label{eq:kde-mixture}
    \widehat{p}_{h}(\mathbf{x}, y)
    \;=\;
    \frac{1}{n}\sum_{i=1}^n 
    \mathcal{N}\!\Bigl(
      (\mathbf{x}_i, y_i),\;
      h^2 \mathbf{I}
    \Bigr),
\end{equation}
\noindent
where each term 
$\mathcal{N}\!\bigl((\mathbf{x}_i, y_i),\,h^2\mathbf{I}\bigr)$ 
denotes a \emph{$(d+1)$-dimensional multivariate normal distribution} 
with mean vector $(\mathbf{x}_i, y_i)$ 
and covariance matrix $h^2 \mathbf{I}$. 
This construction ensures that our KDE in $\mathbb{R}^{d+1}$ 
smoothly captures both features and targets around each real data point.


\begin{flushleft}
\textbf{Note on kernel choice:} Although the Gaussian kernel is the most common, the same 
framework can be extended to other kernel families (e.g., Epanechnikov, Laplacian). The crucial 
point is that the kernel be integrable and that its bandwidth parameter(s) be properly tuned. 
This flexibility allows one to adapt the local shape of the density estimate to different data 
characteristics, so long as we can still sample from it.
\end{flushleft}

A sample from \eqref{eq:kde-mixture} can be generated by:
\begin{enumerate}
    \item \textbf{Component Selection:} pick an index $i$ uniformly at random from $\{1,\dots,n\}$.
    \item \textbf{Gaussian (or Chosen Kernel) Draw:} sample $\bigl(\tilde{\mathbf{x}}, \tilde{y}\bigr)$
    from $\mathcal{N}\bigl((\mathbf{x}_i, y_i),\;h^2\mathbf{I}\bigr)$ or another valid kernel-based
    distribution of choice.
\end{enumerate}

\paragraph{Ensuring Subrange Consistency.}
We only fit the KDE on $(\mathbf{x}_i, y_i)$ where $y_i \in [y_{L},\,y_{U}]$. This local region
captures the specific conditional distribution relevant to that portion of the target space.
Hence, new samples will remain \emph{close} (in $d+1$ dimensional space) to real points that
actually appear in that subrange. This preserves continuity in both $\mathbf{x}$ and $y$,
adhering to the intuition that any synthetic target $\tilde{y}$ near $[y_{L},\,y_{U}]$ 
should be compatible with a plausible feature vector $\tilde{\mathbf{x}}$. Although our local KDEs are trained on $y$ values restricted to $[y_L, y_U]$, 
the unbounded support of Gaussian kernels may occasionally produce out-of-range targets. 
To address this, we enforce a ``hard boundary'' by clipping any generated targets 
that lie outside $[y_L, y_U]$. This ensures that no synthetic points 
extend beyond the intended sub-distribution, thereby preserving strict local consistency 
within each segment of the target space. 
strict consistency.

\paragraph{Complete Oversampling Procedure for Each Subrange.}
\begin{enumerate}
    \item \textbf{Subset Data:} Filter the training set to $\mathcal{S}$ where $y_i \in [y_{L},\,y_{U}]$.
    \item \textbf{Fit KDE:} Estimate $\widehat{p}_{h}(\mathbf{x}, y)$ in $\mathbb{R}^{d+1}$ as per
    \eqref{eq:kde}. Bandwidth $h$ is tuned by a small validation procedure 
    (Section~\ref{sec:results}).
    \item \textbf{Decide Oversampling Size:} If $n_{\mathrm{orig}}$ is the size of $\mathcal{S}$,
    we choose a multiplier $m>1$ and set $N_{\mathrm{new}}=\lfloor m\,n_{\mathrm{orig}}\rfloor - n_{\mathrm{orig}}$ 
    additional points to generate.
    \item \textbf{Sample:} Draw $N_{\mathrm{new}}$ points from \eqref{eq:kde-mixture}, combine them
    with the original $\mathcal{S}$ to form an oversampled sub-dataset, 
    $\tilde{\mathcal{S}} = \mathcal{S} \cup \{\text{new draws}\}$.
\end{enumerate}

\subsubsection{Why Joint (X,y) KDE Preserves Local Nuances}

A core advantage of learning $\widehat{p}_{h}(\mathbf{x}, y)$ in each subrange separately is that
the resulting oversampling respects \emph{all} nuances within that interval. For instance,
if in $[y_{L},\,y_{U}]$ there exist distinct clusters in feature space, the Gaussian (or other
kernel) mixture in \eqref{eq:kde-mixture} will naturally place synthetic points near each cluster 
center. Likewise, correlated patterns between certain feature dimensions and $y$ are 
\emph{locally} captured by the kernel’s smoothing around real data. In contrast, a global model 
that tries to learn $p(\mathbf{x}, y)$ for the entire range of $y$ might obscure minority 
structures or annihilate subtle modes corresponding to rare targets. By zooming in on one slice 
of the target range at a time, then combining these oversampled slices, WMDO obtains a more faithful 
overall representation of the full $(\mathbf{x}, y)$ distribution.

\medskip
\noindent
\textbf{Summary of Mathematical Steps:}
\begin{enumerate}
    \item \textbf{Local Subset}: Restrict training data to $y \in [y_{L},\,y_{U}]$.
    \item \textbf{Joint Density Estimation in $\mathbb{R}^{d+1}$}:
          \[
             \widehat{p}_{h}(\mathbf{x}, y)
             \;=\; 
             \frac{1}{n} \sum_{i=1}^n \mathcal{N}\!\bigl( (\mathbf{x}_i, y_i),\,h^2\,\mathbf{I}\bigr).
          \]
    \item \textbf{Sampling New Points}:
          \[
             (\tilde{\mathbf{x}}, \tilde{y}) \sim \widehat{p}_{h}(\cdot)
             \;\rightarrow\;
             \text{add to oversampled set.}
          \]
    \item \textbf{Composite Dataset}:
          \[
             \tilde{\mathcal{S}}
             \;=\;
             \mathcal{S} \,\cup\,
             \bigl\{(\tilde{\mathbf{x}}, \tilde{y})_1,\dots,(\tilde{\mathbf{x}}, \tilde{y})_{N_{\mathrm{new}}}\bigr\}.
          \]
\end{enumerate}



\noindent
\subsection{5.6\quad Merging the conditional sub-distributions into a final oversampled dataset}
\label{sec:concat_conditional_subdists}

After performing the joint KDE-based oversampling on each local distribution of the target variable, we obtain multiple \emph{oversampled} subsets, one per interval of the fitted distribution’s PPF (Section~5.5). Each local distribution contains both the original points within that subrange and any newly generated synthetic points. The next step is to \emph{concatenate} these oversampled subsets into a unified training set. This final oversampled dataset preserves information across all target intervals while supplying additional samples where they are most needed (i.e., in sparse or tail regions).

\subsubsection{Rationale for Merging}

\begin{itemize}
    \item \textbf{Local Fidelity}: By oversampling each subrange independently, we ensure that synthetic points in one region of the target space do not distort a different region’s distribution. This maintains local fidelity to the empirical data in each interval.
    \item \textbf{Holistic Coverage}: Once the oversampled subsets are combined, the resulting dataset provides comprehensive coverage across the entire range of $y$, mitigating imbalances in rare regions but still retaining an appropriate number of points in well-populated areas.
    \item \textbf{Flexibility}: Because each local distribution's oversampling ratio (or density bandwidth) can be tuned separately (Sections~5.3 and 5.5), we can give more emphasis to intervals where extreme or minority targets occur, without having to over-inflate other intervals where data is already sufficient.
\end{itemize}


Let $M$ denote the number of intervals along the target axis, as determined by the distribution-based partitioning (Section~5.3). For $1 \le i \le M$, we define:

\[
  \mathcal{S}_{i}
  \;=\;
  \bigl\{\,
  (\mathbf{x}_{j}, y_{j})
  : y_{j} \in [y_{L}^{(i)},\, y_{U}^{(i)}],
  \quad j = 1,\dots,n
  \bigr\},
\]
as the set of original training points with targets in the $i$-th subrange \(\bigl[y_{L}^{(i)},\,y_{U}^{(i)}\bigr]\). Each subrange is oversampled to produce
\[
  \widetilde{\mathcal{S}}_{i} 
  \;=\;
  \mathcal{S}_{i}
  \;\cup\;
  \{\text{synthetic samples drawn from } \widehat{p}_{h_i}(\mathbf{x}, y)\}.
\]
Here, $\widehat{p}_{h_i}(\mathbf{x}, y)$ is the joint KDE estimated from $\mathcal{S}_{i}$ (Section~5.5), using a locally chosen bandwidth $h_i$ and an oversampling multiplier $m_i$.

Finally, we combine these oversampled subsets into one dataset:
\begin{equation}
  \label{eq:overall_oversample_union}
  \widetilde{\mathcal{S}}_{\text{full}}
  \;=\;
  \bigcup_{i=1}^{M}
  \widetilde{\mathcal{S}}_{i}.
\end{equation}
In practice, this is simply the concatenation of all $(\mathbf{x}, y)$ pairs (both real and synthetic) from each subrange’s output. If some intervals received no oversampling (e.g., because they were already sufficiently populated), we simply keep the original points in that interval.

\subsubsection{Important Considerations}

\paragraph{Overlap between Adjacent Subranges.}  
Because we define subranges via the fitted CDF’s \emph{percentile points} (Section~5.3), the intervals $[\,y_{L}^{(i)},\,y_{U}^{(i)}\bigr)$ typically do not overlap. However, if there is any boundary fuzziness (for instance, due to floating-point or numerical approximations), duplicates are possible at interval edges. In most use-cases, duplicates do not pose a major issue—models can interpret them as repeated samples—but if desired, one can remove identical duplicates or apply small perturbations to boundary points.

\paragraph{Consistency in Feature Space.}  
Each subrange’s KDE is trained only on points from its corresponding $y$ interval, so the newly synthesized points will predominantly cluster near real observations that share similar target values. Concatenating all intervals preserves these local clusters without merging them artificially. The resulting dataset accurately reflects the multi-modal or skewed structure of the original data.

\paragraph{Handling Large Multipliers.}  
If an interval is extremely sparse, one might choose a high oversampling ratio $m_i \gg 1$. It is good practice to limit $m_i$ (for instance, to some maximum) to avoid excessively enlarging a subrange that contains only a few real points. Alternatively, one may tune $m_i$ via validation metrics (Sections~5.3 and~5.5) to ensure that each subrange’s oversampling genuinely improves predictive performance.

\paragraph{Final Usage in Model Training.}  
The unified oversampled set $\widetilde{\mathcal{S}}_{\text{full}}$ (Eq.~\ref{eq:overall_oversample_union}) now serves as the training data for any standard regression model (or for further splitting into train/validation sets). By construction, the learned regressor can focus more effectively on underrepresented targets, as those ranges are populated by sufficient synthetic samples.

\subsubsection{Algorithmic Recap}
\begin{enumerate}
    \item \textbf{Partition Target Range}: 
    Use the distribution’s PPF (Section~5.2) to define $M$ intervals $\bigl\{[y_{L}^{(i)}, y_{U}^{(i)}]\bigr\}_{i=1}^{M}$, ensuring each subrange captures approximately equal probability mass and meets a minimum sample criterion.
    \item \textbf{Local KDE Estimation}: 
    For each interval $i$, select the real points $\mathcal{S}_{i}$ and fit a kernel density estimator $\widehat{p}_{h_i}(\mathbf{x}, y)$ in $\mathbb{R}^{d+1}$. Tune the bandwidth $h_i$ and oversampling multiplier $m_i$ based on local validation.
    \item \textbf{Synthesize Points}: 
    Draw $\bigl\lfloor (m_i - 1)\,\lvert \mathcal{S}_{i}\rvert \bigr\rfloor$ new samples from $\widehat{p}_{h_i}(\mathbf{x}, y)$, clipping any out-of-range targets to $[y_{L}^{(i)}, y_{U}^{(i)}]$.
    \item \textbf{Concatenate}: 
    Form $\widetilde{\mathcal{S}}_{i} = \mathcal{S}_{i} \cup \{\text{synthetic}\}$, then merge across all $i$ to obtain $\widetilde{\mathcal{S}}_{\text{full}} = \bigcup_{i=1}^{M} \widetilde{\mathcal{S}}_{i}$.
    \item \textbf{Model Training}: 
    Train the desired regressor on $\widetilde{\mathcal{S}}_{\text{full}}$, benefiting from improved balance and richer coverage in sparse (tail) regions.
\end{enumerate}

\noindent
\textbf{Outcome.} By unifying these local, properly oversampled sub‐distributions, WMDO effectively merges them into a final oversampled dataset. This preserves each subrange’s unique feature–target relationships while addressing the global imbalance. The result is a single, coherent training set that leads to stronger performance across the entire target spectrum, avoiding the pitfalls of arbitrary thresholding or excessive interpolation in standard oversampling methods.







\begin{figure}[!t]
    \vspace{-1cm}   % Adjust to suit
    \centering
    \includegraphics[width=\textwidth,
    height=0.1\textheight,
    ]{Model the Train Target’s Distribution (2).png}
    \caption{WMDO flow chart.}
    \label{fig:method_large_image}
\end{figure}




% ------------------------
%  EXPERIMENTAL RESULTS
% ------------------------
\section{Research methodology}
\label{sec:rmethod}


% ------------------------
% IMPLEMENTATION DETAILS
% ------------------------
\section{Results}
\label{sec:results}


% ------------------------
%     REFERENCES
% ------------------------
\begin{thebibliography}{99}

% Reordered according to appearance in text (numeric labels).
% ----------------------------------------------------------

%1
\bibitem{he2009}
H.~He and E.~A.~Garcia, “Learning from imbalanced data,” \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~21, no.~9, pp. 1263--1284, 2009.
%2
\bibitem{haixiang2017} H.~Guo, Y.~Li, J.~Shang, M.~Gu, Y.~Huang, and B.~Gong, “Learning from class-imbalanced data: Review of methods and applications,” \emph{Expert Systems with Applications}, vol.~73, pp. 220--239, 2017.
%3
\bibitem{chawla2002}
N.~V. Chawla, K.~W. Bowyer, L.~O. Hall, and W.~P. Kegelmeyer,
“SMOTE: Synthetic minority over-sampling technique,”
\emph{Journal of Artificial Intelligence Research}, vol.~16, pp. 321--357, 2002.
%4
\bibitem{buda2018}
M.~Buda, A.~Maki, and M.~A.~Mazurowski, “A systematic study of the class imbalance problem in convolutional neural networks,” 
\emph{Neural Networks}, vol.~106, pp. 249--259, 2018.
%5
\bibitem{johnson2019}
J.~M.~Johnson and T.~M.~Khoshgoftaar, “Survey on deep learning with class imbalance,”
\emph{Journal of Big Data}, vol.~6, no.~1, pp. 1--54, 2019.

%6
\bibitem{liu2009}
X.-Y.~Liu, J.~Wu, and Z.-H.~Zhou, “Exploratory undersampling for class-imbalance learning,”
\emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, vol.~39, no.~2, pp. 539--550, 2009.

%7
\bibitem{krawczyk2016}
B.~Krawczyk, “Learning from imbalanced data: open challenges and future directions,”
\emph{Progress in Artificial Intelligence}, vol.~5, no.~4, pp. 221--232, 2016.


%8
\bibitem{branco2016}
P.~Branco, L.~Torgo, and R.~P.~Ribeiro, “A survey of predictive modeling under imbalanced distributions,”
\emph{ACM Computing Surveys}, vol.~49, no.~2, Article 31, 2016.

%9
\bibitem{torgo2013}
L.~Torgo, R.~P. Ribeiro, J.~P. da Costa, and S.~Pal,
“SMOTE for regression,” in
\emph{Intelligent Data Engineering and Automated Learning (IDEAL 2013). Lecture Notes in Computer Science}, vol.~8206, 2013, pp. 378--387.

%10
\bibitem{chawla2004}
N.~V.~Chawla, N.~Japkowicz, and A.~Kolcz, “Editorial: Special issue on learning from imbalanced data sets,”
\emph{ACM SIGKDD Explorations Newsletter}, vol.~6, no.~1, pp. 1--6, 2004.

%11
\bibitem{kaur24}
A.~Kaur and M.~Sarmadi, “Comparative analysis of machine learning techniques
for imbalanced genetic data,” \emph{Annals of Data Science}, 2024.

%12
\bibitem{scheepens23}
D.~Scheepens, I.~Schicker, K.~Hlaváčková-Schindler, and C.~Plant,
“Adapting a deep convolutional RNN model with imbalanced regression loss
for improved spatio-temporal forecasting of extreme wind speed events 
in the short to medium range,” \emph{Geosci. Model Dev.}, 2023.

%13
\bibitem{xu2022}
Z.~Xu, C.~Zhao, C.~D.~Scales~Jr, R.~Henao, and B.~A.~Goldstein,
“Predicting in-hospital length of stay: a two-stage modeling approach to account for highly skewed data,” 
\emph{BMC Medical Informatics and Decision Making}, vol.~22, article~110, 2022.

%14
\bibitem{branco2017}
P.~Branco, L.~Torgo, and R.~P. Ribeiro,
“SMOGN: A pre-processing approach for imbalanced regression,” in
\emph{Proceedings of Machine Learning Research: LIDTA}, vol.~74, 2017, pp. 36--50.

%15
\bibitem{steininger2021}
M.~Steininger, K.~Kobs, P.~Davidson, A.~Krause, and A.~Hotho, 
“Density-based weighting for imbalanced regression,”
\emph{Machine Learning}, vol. 110, no. 8, pp. 2187--2210, 2021.

%16
\bibitem{he2008}
H.~He, Y.~Bai, E.~A. Garcia, and S.~Li, “ADASYN: Adaptive synthetic sampling approach for imbalanced learning,” in
\emph{Proceedings of the International Joint Conference on Neural Networks}, 2008, pp. 1322--1328.

%17
\bibitem{han2005}
H.~Han, W.-Y.~Wang, and B.-H.~Mao, “Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning,” in
\emph{Proceedings of ICIC 2005}, 2005, pp. 878--887, Springer.

%18
\bibitem{bunkhumpornpat2012}
C.~Bunkhumpornpat, K.~Sinapiromsaran, and C.~Lursinsap, “DBSMOTE: Density-based synthetic minority over-sampling technique,” \emph{Applied Intelligence}, vol.~36, no.~3, pp.~664--684, 2012.


%19
\bibitem{he2013}
H.~He and Y.~Ma,
\emph{Imbalanced learning: Foundations, algorithms, and applications}.
John Wiley \& Sons, 2013.

%20
\bibitem{torgo2007}
L.~Torgo and R.~P.~Ribeiro, “Utility-based regression,” in
\emph{Proceedings of PKDD 2007}, 2007, pp.~597--604, Springer.

%21
\bibitem{ribeiro2011a}
R.~P.~A.~Ribeiro, \emph{Utility-based regression} (Ph.D. thesis), Porto: Faculty of Sciences, University of Porto, 2011.

%22
\bibitem{torgo2015}
L.~Torgo, P.~Branco, R.~P.~Ribeiro, and B.~Pfahringer, “Resampling strategies for regression,” \emph{Expert Systems}, vol.~32, no.~3, pp.~465--476, 2015.


%23
\bibitem{branco2019}
P.~Branco, L.~Torgo, and R.~P.~Ribeiro, “Pre-processing approaches for imbalanced distributions in regression,”
\emph{Neurocomputing}, vol.~343, pp.~76--99, 2019.

%24
\bibitem{camacho2022}
L.~Camacho, G.~Douzas, and F.~Bacao,
“Geometric SMOTE for regression,”
\emph{Expert Systems with Applications}, vol. 193, 116387, 2022.

%25
\bibitem{zhou2010}
Z.-H.~Zhou and X.-Y.~Liu, “On multi-class cost-sensitive learning,”
\emph{Computational Intelligence}, vol.~26, no.~3, pp.~232--257, 2010.

%26
\bibitem{elkan2001}
C.~Elkan, “The foundations of cost-sensitive learning,” in
\emph{Proceedings of the 17th International Joint Conference on Artificial
Intelligence (IJCAI)}, 2001, pp. 973--978.

%27
\bibitem{domingos1999}
P.~Domingos, “MetaCost: A general method for making classifiers
cost-sensitive,” in \emph{Proceedings of the 5th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD)}, 1999, pp. 155--164.

%28
\bibitem{steininger2021}
M.~Steininger, K.~Kobs, P.~Davidson, A.~Krause, and A.~Hotho, “Density-based weighting for imbalanced regression,”
\emph{Machine Learning}, vol.~110, no.~8, pp.~2187--2211, 2021.

%29
\bibitem{yang2021} J. Yang, L. Xie, Q. Yu, X. He, and J. Liu, “Delving into deep imbalanced regression,” in \emph{Proceedings of the 38th International Conference on Machine Learning (ICML)}, pp. 8437-8447, 2021.

%30
\bibitem{ren2022} M. Ren, W. Luo, and R. Urtasun, “Balanced MSE for imbalanced visual regression,” in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 418-427, 2022.

%31
\bibitem{moniz2018} N. Moniz, L. Torgo, and C. Soares, “SMOTEBoost for regression: Improving the prediction of extreme values,” in \emph{Proceedings of the 5th International Conference on Data Science and Advanced Analytics (DSAA)}, pp. 127-136, 2018.

%32
\bibitem{hoens2013}
T.~R.~Hoens and N.~V.~Chawla, “Imbalanced datasets: From sampling to classifiers,” in
\emph{Imbalanced Learning: Foundations, Algorithms, and Applications}, Wiley, 2013, pp.~43--59.

%33
\bibitem{ribeiro2020}
R.~P.~Ribeiro and N.~Moniz, “Imbalanced regression and extreme value prediction,”
\emph{Machine Learning}, vol.~109, no.~9--10, pp.~1803--1835, 2020.




\end{thebibliography}

\end{document}
