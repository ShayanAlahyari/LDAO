\documentclass[10pt]{article}

% ------------------------------------------------
%                  PACKAGES
% ------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{amsmath,amssymb}
\usepackage{float} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xcolor}
\usepackage{pdflscape}
\usepackage{caption} 
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage{makecell}   % For multi-line cells
\usepackage{colortbl}   % For header row background
\usepackage{caption}
\usepackage{subcaption}
\usepackage[x11names]{xcolor}

\captionsetup[subfigure]{labelformat=empty}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkred}{rgb}{0.5,0,0}

% ------------------------------------------------
%           HYPERREF CONFIGURATION
% ------------------------------------------------
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue,
  pdfborder={0 0 0}
}

% -----------------------------
%  SINGLE-COLUMN PAGE SETUP
% -----------------------------
\usepackage[margin=0.8in]{geometry}


\begin{document}

\title{Local distribution-based adaptive oversampling for imbalanced regression}
\author{
}

\date{}
\maketitle

% ------------------------
%      ABSTRACT
% ------------------------



\begin{abstract}
Imbalanced regression occurs when continuous target variables have skewed distributions, creating sparse regions that are difficult for machine learning models to predict accurately. This issue particularly affects neural networks, which often struggle with imbalanced data. While class imbalance in classification has been extensively studied, imbalanced regression remains relatively unexplored, with few effective solutions. Existing approaches often rely on arbitrary thresholds to categorize samples as rare or frequent, ignoring the continuous nature of target distributions. These methods can produce synthetic samples that fail to improve model performance and may discard valuable information through undersampling. To address these limitations, we propose LDAO (Local Distribution-based Adaptive Oversampling), a novel data-level approach that avoids categorizing individual samples as rare or frequent. Instead, LDAO learns the global distribution structure by decomposing the dataset into a mixture of local distributions, each preserving its statistical characteristics. LDAO then models and samples from each local distribution independently before merging them into a balanced training set. LDAO achieves a balanced representation across the entire target range while preserving the inherent statistical structure within each local distribution. In extensive evaluations on 45 imbalanced datasets, LDAO outperforms state-of-the-art oversampling methods on both frequent and rare target values, demonstrating its effectiveness for addressing the challenge of imbalanced regression.
\end{abstract}

% ------------------------
%    INDEX TERMS
% ------------------------
\begin{flushleft}
\textbf{Keywords} Imbalanced, Regression, Kernel density estimation, Oversampling, Data-level, Supervised learning, local distribution.
\end{flushleft}

% ------------------------
%     INTRODUCTION
% ------------------------
\section{Introduction}


In classification tasks, imbalance occurs when some classes have far fewer samples than others, in both binary and multi-class settings \cite{he2009, haixiang2017}. The minority classes are overshadowed by the majority classes, causing traditional classifiers to focus on the more abundant classes and perform poorly in identifying minority classes \cite{chawla2002}. This leads to inaccurate detection of minority classes since models struggle to recognize patterns in sparsely sampled regions \cite{buda2018}. Critical applications like fraud detection, medical diagnostics, and fault detection are particularly affected, as rare but important classes might be missed \cite{johnson2019}. Therefore, effective handling of imbalanced data is crucial for reliable performance across all classes, ensuring significant rare examples are properly detected \cite{liu2009}.

\smallskip

While imbalance is widely studied in classification \cite{he2009}, it also affects regression tasks that predict continuous values \cite{krawczyk2016}. Imbalanced regression occurs when certain target ranges (typically rare or extreme cases) have significantly fewer samples than others \cite{branco2016, torgo2013}. Traditional regression models struggle to accurately predict these rare values because they focus on more frequent, well-represented ranges \cite{chawla2004, branco2016}. Accurately predicting these less frequent target ranges is challenging yet crucial for many real-world applications \cite{torgo2013}.

\smallskip

Figure~\ref{fig:sparse_region} contrasts imbalanced classification with imbalanced regression. In classification, identifying minority classes is straightforward because labels are discrete and can be counted. In regression, minority regions exist within continuous target distributions, often in sparsely populated tails. While classification imbalance relates to distinct labels, regression imbalance depends on distribution shape (skewed or multi-modal). Classification methods cannot be directly applied to regression since there is no simple way to count labels in order to identify underrepresented values. This makes detecting and addressing sparse regions a significant challenge in imbalanced regression.

\smallskip


Imbalanced regression impacts many critical real-world applications. In genomic prediction, pathogenicity scores typically favor certain ranges, placing potentially harmful genetic variants in sparse distribution tails. Standard models struggle to predict these rare but important extremes \cite{kaur24}. 



\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imbalance_problem.pdf}
\caption{\footnotesize The left image shows a classification problem with easily identifiable minority (red) and majority (green) classes. The right image illustrates an imbalanced regression problem where target values in the sparse region (red regions) are underrepresented, making them more difficult to detect and accurately predict.}
    \label{fig:sparse_region}
\end{figure}


For extreme wind event forecasting, measurements primarily cluster around moderate speeds, while infrequent high-speed events critical for hazard warnings and energy grid stability are often underestimated by conventional models that prioritize majority data \cite{scheepens23}. In healthcare, hospital length-of-stay data is heavily skewed toward short stays, with fewer patients requiring extended care. Models trained predominantly on brief stays underestimate resource needs for longer admissions, affecting capacity planning and staffing, particularly during crises like pandemics \cite{xu2022}. Similar challenges exist in economics, finance, and engineering, where accurately predicting rare but high-impact events is essential for effective decision-making.

\smallskip

Several solutions have been proposed to address imbalanced regression at both data and algorithmic levels. Some extend SMOTE from classification to regression by generating synthetic examples in the target distribution's tails, increasing representation of rare values \cite{torgo2013,branco2017}. Others apply cost-sensitive methods that penalize errors on underrepresented values more heavily, encouraging models to focus on these instances \cite{steininger2021}. Despite showing promise, the current solutions for imbalanced regression remain limited, and more robust approaches are needed.

\smallskip

We propose LDAO, a novel data-level oversampling approach that addresses imbalanced regression through adaptive region-specific sampling. LDAO preserves the continuous nature of regression targets while effectively enhancing representation across the entire target distribution. Our approach provides a more robust foundation for regression models when dealing with imbalanced target distributions.


% ------------------------
%     RELATED WORK
% ------------------------
\section{Related Work}
\label{sec:relatedwork}

Early approaches to imbalanced data focused on data-level methods, particularly in classification, and data-level strategies appear in nearly one-third of imbalanced classification papers, though most were designed for discrete classes \cite{haixiang2017}. These methods rebalance datasets by either adding minority samples or removing majority samples \cite{chawla2002,he2008}. SMOTE exemplifies this approach by generating synthetic minority samples through interpolation in feature space \cite{chawla2002}. It works by selecting a minority sample, finding its k-nearest minority neighbors, and creating new points by randomly interpolating between their feature values. This technique helps distribute minority samples more evenly throughout the data space.

\smallskip

Several SMOTE variations have emerged to address specific challenges. Borderline-SMOTE focuses on minority samples near class boundaries \cite{han2005}, while DBSMOTE leverages local density information to generate synthetic points in regions where minority classes are concentrated \cite{bunkhumpornpat2012}.~These methods aim to improve minority class coverage and enhance classifier performance on imbalanced datasets.

\smallskip

Adapting data-level strategies to regression is more difficult because the target variable is continuous \cite{he2013, krawczyk2016}. Early approaches to imbalanced regression modified classification resampling by introducing a relevance function $\phi(y)$ to distinguish between "rare" and "frequent" target regions \cite{torgo2007}. This function assigns higher scores to more extreme target values, while a user-specified threshold determines which points are rare versus common \cite{torgo2007, ribeiro2011a}. Using this relevance function framework, Torgo et al.~developed two main resampling methods for regression. 

\smallskip

The first method applies random undersampling to remove samples with low $\phi(y)$ scores (common target values), reducing their overrepresentation. The second method, SMOTER, generates new samples in regions with high $\phi(y)$ scores (rare target values) by interpolating feature values of nearby rare samples and calculating target values as weighted averages of these neighbors \cite{torgo2013, torgo2015}. While these approaches were pioneering in addressing regression imbalance and demonstrated that focusing on rare targets improves performance \cite{torgo2015}, they depend on user-chosen rarity thresholds that may be arbitrary and fail to reflect the true structure of continuous target distributions \cite{branco2019}.

\smallskip

SMOGN (Synthetic Minority Over-sampling Technique for Regression with Gaussian Noise) extends SMOTER by combining interpolation with noise-based oversampling. For each rare instance, SMOGN generates either an interpolated synthetic point (as in SMOTER) or adds Gaussian noise to create a perturbed point \cite{branco2017}. This hybrid approach provides more flexible oversampling of sparse regions and outperforms SMOTER \cite{branco2017}. SMOGN also applies undersampling to reduce overrepresented frequent examples, establishing it as a state-of-the-art resampling method for imbalanced regression \cite{branco2017}.

\smallskip

Additional oversampling approaches include simple random oversampling (replicating high-$\phi$ examples) and pure noise-based oversampling (adding small random perturbations to rare examples) \cite{branco2019}. Another method, WERCS (WEighted Relevance-based Combination Strategy), probabilistically selects instances for oversampling or undersampling based on relevance scores: rare instances have higher chances of duplication while common instances are typically undersampled unless kept for diversity. This unified approach blends techniques to avoid hard threshold cutoffs \cite{branco2019}.

\smallskip

More recently, Camacho et al.~developed Geometric SMOTE (G-SMOTE) for regression, adapting geometrically enhanced SMOTE to continuous targets \cite{camacho2022}. G-SMOTE generates synthetic samples in minority regions by considering both feature-space interpolation and target distribution geometry, offering greater flexibility in sample placement \cite{camacho2022}.

\smallskip

Stocksieker et al.~combined weighted resampling and data augmentation to better represent covariate distributions and reduce overfitting \cite{stocksieker2023}. Their approach weights underrepresented regions while using noise addition or interpolation to cover wider data ranges. Camacho and Bacao introduced WSMOTER (Weighting SMOTE for Regression), replacing fixed thresholds with instance-based weighting to better highlight underrepresented targets and improve synthetic sample generation in sparse regions \cite{camacho2024}. Stocksieker et al.~later developed GOLIATH, a framework handling both noise and interpolation methods with hybrid generators and wild-bootstrap steps for continuous targets \cite{stocksieker2024}. Aleksic and García-Remesal proposed SUS (Selective Under-Sampling), which selectively removes only those majority samples that provide little additional information, along with an iterative variant (SUSiter) that reintroduces discarded samples over time, showing strong results on high-dimensional datasets \cite{aleksic2025}.

\smallskip

In addition to data-level strategies, algorithm-level solutions embed imbalance handling directly into the learning process. Cost-sensitive learning exemplifies this approach by modifying loss functions or instance weights to make models focus on rare targets and incur higher penalties for errors on these examples. This concept originated in classification by weighting classes inversely to their frequency and extends naturally to regression by emphasizing rare or extreme target values \cite{zhou2010,elkan2001,domingos1999}. By imposing higher costs on underrepresented outcomes, these approaches drive models to reduce errors where accuracy matters most.

\smallskip

DenseLoss is a cost-sensitive approach designed specifically for imbalanced regression \cite{steininger2021}. It uses DenseWeight, a density-based weighting strategy that assigns higher weights to samples with less frequent target values. This directs the model to focus on rare or extreme cases where accuracy is most critical \cite{steininger2021}. DenseWeight estimates the target distribution's probability density and gives larger weights to low-density targets, scaling the loss function during training to emphasize prediction accuracy on rare cases. Unlike oversampling methods, DenseLoss modifies the learning process without creating or removing examples from the training set.

\smallskip

Yang et al.~proposed a framework for deep imbalanced regression that improves prediction performance for extreme target values using label-distribution smoothing and adaptive sampling \cite{yang2021}. Their work demonstrates how continuous target imbalance can severely impact deep neural network performance. Ren et al.~introduced Balanced MSE, which modifies standard mean-squared error to give higher weight to errors on rare targets \cite{ren2022}. Validated across multiple computer vision tasks, their approach shows that directly addressing label imbalance outperforms standard MSE in skewed settings.

\smallskip

While cost-sensitive approaches like DenseLoss show promise by avoiding synthetic data augmentation \cite{steininger2021}, they remain relatively uncommon in imbalanced regression.~A key challenge is their sensitivity to the weighting function—optimization can become unstable if rare cases are weighted too heavily \cite{he2009}.

\smallskip

Ensemble learning has been explored for imbalanced regression challenges. While boosting and bagging improve model generalization by combining multiple learners, they don't directly address skewed distributions \cite{hoens2013}. Therefore, researchers typically combine ensemble methods with specific imbalance-countering techniques. One effective approach applies resampling within each ensemble iteration. Resampled Bagging, for instance, uses balanced subsets or weighted samples in each bootstrap replicate, preserving diversity across learners while reducing bias toward majority targets \cite{branco2019}. 

\smallskip

Moniz et al.~extended boosting for rare targets through SMOTEBoost for regression, which generates synthetic samples in extreme target ranges and then uses boosting to emphasize these difficult-to-predict points \cite{moniz2018}. This combination of oversampling and ensemble methods proves especially valuable when models need to accurately capture tail values or outliers that standard regression approaches frequently miss.

\smallskip

Evaluation metrics for imbalanced regression have evolved alongside data-level, algorithmic-level, and ensemble-based solutions. Traditional metrics like MSE or MAE are often dominated by errors on frequent cases, prompting the development of specialized metrics that better assess performance on rare targets. The SERA metric (Squared Error Relevance Area) exemplifies this approach by incorporating relevance into evaluation, weighting errors according to the rareness of target values \cite{ribeiro2020}. Ribeiro and Moniz argue that SERA offers more informative model comparisons in imbalanced domains by highlighting performance on extreme values relative to common ones \cite{ribeiro2020}. While SERA itself is an evaluation metric rather than a training method, its development reflects the field's increasing focus on accurately assessing how well models handle rare events.



\section{Motivation}
\label{sec:motivation}

Despite recent advances, imbalanced regression still faces significant challenges. Current methods often identify samples as rare or frequent and oversample the distribution based on that assumption. In many cases, these assumptions might be oversimplifications due to the complexity of continuous distributions and might not help the model generalize better or improve its prediction on rare samples. Also, many approaches try to oversample rare targets and undersample frequent targets, which distorts the nature of the dataset. Removing information from datasets and labeling certain ranges as rare or frequent is problematic, as rare samples can occur in any range within the global distribution, especially in multi-modal distributions. Often, distributions have latent characteristics which make it difficult to simply classify which samples are rare or frequent.

\smallskip

In addition, interpolation and noise-based oversampling techniques may generate synthetic samples that don't reflect actual data patterns, especially in extremely sparse regions \cite{branco2019}. Creating synthetic feature-target pairs that remain consistent with the original distribution is challenging, and most methods focus narrowly on extreme tails rather than the entire distribution.~Many approaches rely heavily on relevance functions and thresholds that require careful parameter selection.~Poor choices can lead to oversampling marginally rare areas while missing truly rare regions. This dependence on domain-specific tuning limits broader applicability across different datasets.
Real-world applications in healthcare, genomics, environmental science, finance, and engineering frequently involve imbalanced regression tasks, and we need more robust approaches that tackle the problem of imbalanced regression as we have seen in classification. 

\smallskip

Adaptive oversampling techniques in classification, such as the method by Wang et al.\cite{wang2020}, which leverages local distribution information to generate synthetic minority instances, demonstrate the potential of these strategies. However, these methods rely on discrete class labels and fixed thresholds that do not translate well to regression tasks, where the target variable is continuous.

\smallskip

Our method, LDAO, addresses these limitations by preserving data structure across all target ranges. Instead of using global thresholds or discarding frequent samples, LDAO augments each cluster of the target distribution independently, generating synthetic data that match local distribution patterns. Our goal in LDAO is to improve the model’s prediction of rare samples while enhancing its overall generalization. LDAO aims to mitigate the trade-off between emphasizing rare samples and preserving overall generalization ability.






% ------------------------
%   The proposed method
% ------------------------
\section{LDAO Method}
\label{sec:theproposedmethod}

We now describe LDAO, our local distribution-based adaptive oversampling method that effectively addresses the challenges of imbalanced regression. LDAO decomposes the global distribution using \textit{k}-means clustering. This clustering is applied simultaneously to both input features and target values, creating a mixture of local distributions in a joint feature–target space.

\smallskip

After clustering, we apply Gaussian kernel density estimation (KDE) within each cluster to model the local distribution independently and generate synthetic data points that closely match the cluster’s actual distribution. By drawing samples independently within each cluster, LDAO avoids creating unrealistic synthetic points. Finally, we merge these augmented clusters into a single augmented dataset. The entire workflow is summarized in Figure~\ref{fig:method_large_image}: data clustering, local density estimation for synthetic data generation, and merging to achieve an augmented overall distribution.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LDAO.pdf}
\caption{\footnotesize LDAO process overview. First, the imbalanced dataset (top) is decomposed into clusters. Then, each cluster is oversampled individually using kernel density estimation (middle). Lastly, these balanced clusters are combined into one dataset (bottom).}
    \label{fig:method_large_image}
\end{figure}
Given a dataset \(\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^N\), where \(\mathbf{x}_i \in \mathbb{R}^d\) represents features and \(y_i \in \mathbb{R}\) represents the target variable, imbalance typically manifests as regions with fewer observations, making these areas difficult to model accurately. LDAO models the joint distribution of features and targets. Each data point \((\mathbf{x}_i, y_i)\) is embedded in the combined feature-target space as \(\mathbf{z}_i = (\mathbf{x}_i, y_i)\in \mathbb{R}^{d+1}\). The overall joint distribution \(P_{X,Y}\) is approximated by a mixture model composed of localized distributions:
\[
P_{X,Y}(\mathbf{x}, y) \approx \sum_{k=1}^{K} \pi_k \, P_k(\mathbf{x}, y)
\]
where \(\pi_k\) denotes the proportion of data in cluster \(k\), and \(P_k\) characterizes the local distribution of that cluster.

\subsection{Clustering in the Joint Feature-Target Space}

We apply the standard \(k\)-means clustering algorithm to the dataset to identify natural clusters in the joint feature–target space. Figure~\ref{fig:clustering} illustrates clustering for a sample data set, projected onto the first three principal components, and shows how points with similar features and target values are grouped for more precise local density modelling. For illustration purposes only, we chose 3 as the number of clusters. In this method, every data point \(\mathbf{z}_i\) is assigned to one of \(K\) clusters. The goal is to find the best set of cluster centroids \(\{\boldsymbol{\mu}_k\}_{k=1}^K\) and assignments \(\{c_i\}_{i=1}^N\) such that the total squared Euclidean distance between the data points and their assigned centroids is minimized. Mathematically, we solve:
\[
\underset{\{\boldsymbol{\mu}_k\}_{k=1}^K, \{c_i\}_{i=1}^N}{\arg\min} \; \sum_{i=1}^N \|\mathbf{z}_i - \boldsymbol{\mu}_{c_i}\|^2.
\]
This formulation ensures that points with similar characteristics are grouped together \cite{sugar2003,ikotun2023}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{K-means_clustering.pdf}
\caption{\footnotesize K-means clustering in the joint feature–target space for the Boston dataset \cite{harrison1978hedonic}, projected onto the first three principal components. Data points are grouped into three clusters (each indicated by a unique marker and color). Ellipsoidal density contours characterize the spread and orientation of each cluster. The centroids, marked with prominent gold symbols, represent the mean positions of the data points within their respective clusters.}
    \label{fig:clustering}
\end{figure}


% Many studies, including those by Syakur et al. \cite{syakur2018} and Nainggolan et al. \cite{nainggolan2019}, have successfully used this approach to capture inherent data patterns without applying arbitrary thresholds.

\subsection{Determining the Number of Clusters}

The performance of \(k\)-means clustering largely depends on the chosen number of clusters \(K\). To choose an optimal \(K\), we evaluate the Sum of Squared Errors (SSE), defined as:
\[
\mathrm{SSE}(K) = \sum_{k=1}^{K}\sum_{\mathbf{z}\in\mathcal{D}_k}\|\mathbf{z}-\boldsymbol{\mu}_k\|^2,
\]
where \(\mathcal{D}_k\) represents the set of data points in cluster \(k\). As we increase \(K\), the SSE typically decreases because each point is closer to its cluster center. 

To mitigate the pitfalls of the trade-off between oversimplification and excessive clustering, we use the elbow method \cite{syakur2018,nainggolan2019}. This method involves calculating the relative reduction in SSE when increasing the number of clusters. We define the relative improvement \(\Delta(K)\) as:
\[
\Delta(K) = \frac{\mathrm{SSE}(K-1) - \mathrm{SSE}(K)}{\mathrm{SSE}(K-1)},\quad K=2,\dots,K_{\max}.
\]
The optimal number of clusters $(K^*)$ is typically found where there is a significant drop in $\Delta(K)$. One important note is that at the elbow point, adding more clusters does not considerably reduce the Sum of Squared Errors (SSE) and might cause overfitting by capturing noise rather than true underlying patterns \cite{syakur2018,nainggolan2019}. 

\subsection{Kernel Density Estimation in Each Cluster} 
After the clustering step, we model the local distributions within each cluster separately using kernel density estimation (KDE) \cite{silverman1986}, as illustrated in Figure~\ref{fig:kde}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{KDE.pdf}
  \caption{\footnotesize Clusters obtained via \(k\)-means on the feature-target data are projected onto two principal components. For each cluster, kernel density estimation (KDE) is performed and the overlaid contour lines represent the density gradients of the data in the reduced space.}

  \label{fig:kde}
\end{figure}





This approach preserves each cluster's local structure, maintaining accurate relationships between features and targets without mixing patterns across clusters. Let each cluster $\mathcal{D}_k$ contain $n_k$ data points in the joint feature-target space: 
\[ \mathcal{D}_k = \{\mathbf{z}_1^{(k)},\,\mathbf{z}_2^{(k)},\dots,\mathbf{z}_{n_k}^{(k)}\}, \] 
where each data point is defined as \(\mathbf{z}_j^{(k)} = (\mathbf{x}_j^{(k)},\,y_j^{(k)}) \in \mathbb{R}^{d+1}\). We estimate the local density of cluster \(k\) using KDE with a Gaussian kernel as follows:
\begin{equation}
\hat{f}_k(\mathbf{z}) = \frac{1}{n_k}\sum_{j=1}^{n_k} K(\mathbf{z}-\mathbf{z}_j^{(k)}),
\label{eq:kde}
\end{equation}
where the Gaussian kernel $K(\cdot)$ is defined by: 
\[ K(\mathbf{u}) = \frac{1}{(2\pi)^{\frac{d+1}{2}}|\mathbf{H}|^{1/2}}\exp\left(-\frac{1}{2}\mathbf{u}^\top\mathbf{H}^{-1}\mathbf{u}\right), \quad \mathbf{u}\in\mathbb{R}^{d+1}. \] 
Here, $\mathbf{H}$ is a $(d+1)\times(d+1)$ bandwidth (covariance) matrix that controls the shape and smoothness of the density estimate \cite{scott2015}. The bandwidth matrix $\mathbf{H}$ determines how closely the estimated density follows the observed data points. A smaller determinant $|\mathbf{H}|$ results in sharply peaked density estimates localized around data points, while a larger determinant yields smoother densities.


Selecting an appropriate bandwidth matrix $\mathbf{H}$ is critical to KDE performance \cite{sheather1991}. If $|\mathbf{H}|$ is too small, KDE tends to overfit noise, resulting in unrealistic density estimates. Conversely, an excessively large $|\mathbf{H}|$ oversmooths important local features. Practical approaches such as cross-validation, plug-in estimators, or generalized Silverman's rule for multivariate KDE can be employed independently for each cluster \cite{zhang2006}. By fitting KDE independently to each cluster using carefully selected bandwidth matrices, we create localized models that accurately represent each region's unique distribution. This ensures that synthetic data generated in the subsequent steps reflect genuine local patterns, effectively addressing imbalance without distorting critical data relationships.



\subsection{Oversampling Clusters}

Once the density of each cluster has been estimated, we independently oversample each cluster. Figure~\ref{fig:cluster3} illustrates this process. Each cluster \(k\), originally containing \(n_k\) points, is expanded by generating synthetic data points until reaching a target size defined as:
\[
n_k' = \lceil \alpha_k n_k \rceil,
\]
where the multiplier \(\alpha_k > 1\) is a parameter of the algorithm that determines how much each cluster grows. We generate exactly \(n_k' - n_k\) synthetic points \(\mathbf{z}^*\) drawn from the local KDE-based density estimate \(\hat{f}_k(\mathbf{z})\). The multiplier \(\alpha_k\) can be uniform (identical across clusters) or adaptive (higher for clusters that are smaller or less dense). Typically, smaller or sparser clusters receive a larger multiplier, ensuring balanced representation throughout the dataset. 

Synthetic points \(\mathbf{z}^*\) are generated directly from the KDE-based local distribution defined in Equation~\eqref{eq:kde}:
\[
\mathbf{z}^* \sim \hat{f}_k(\mathbf{z}).
\]
For KDE with a Gaussian kernel using bandwidth matrix \(\mathbf{H}\), synthetic samples are generated by randomly selecting an existing cluster data point and adding a Gaussian perturbation scaled by the Cholesky decomposition of the bandwidth matrix:
\[
\mathbf{z}^* = \mathbf{z}_j^{(k)} + \mathbf{H}^{1/2}\boldsymbol{\epsilon},
\quad\text{where}\quad
j \sim \mathrm{Uniform}\{1,\dots,n_k\},
\quad\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\mathbf{I}_{d+1}).
\]
Here, \(\mathbf{H}^{1/2}\) is the matrix square root (typically via Cholesky decomposition) of the bandwidth covariance matrix \(\mathbf{H}\). This procedure accurately and efficiently samples synthetic points from the KDE estimate, reflecting the local structure and covariance of each cluster\cite{silverman1986,scott2015}.


% \begin{figure}[H]
%     \centering
%     % First subfigure
%     \begin{subfigure}[b]{0.40\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{cluster_1.pdf}
%         % \caption{\footnotesize Cluster 1}
%         \label{fig:cluster1}
%     \end{subfigure}
%     % \hfill
%     % Second subfigure
%     \begin{subfigure}[b]{0.40\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{cluster_2.pdf}
%         % \caption{\footnotesize Cluster 2}
%         \label{fig:cluster2}
%     \end{subfigure}
%     % \hfill
%     % Third subfigure
%     \begin{subfigure}[b]{0.40\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{cluster_3.pdf}
%         % \caption{\footnotesize Cluster 3}
%         \label{fig:cluster3}
%     \end{subfigure}
%     % Global caption for the entire figure
%     \caption{\footnotesize Each cluster is independently oversampled based on its own density estimate. This ensures that both sparse and dense areas are adequately represented without altering their local characteristics.}
%     \label{fig:cluster3}
% \end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{cluster_1.pdf}
  \caption{\footnotesize Each cluster is independently oversampled based on its own density estimate, ensuring that both sparse and dense areas are adequately represented without altering their local characteristics; the same procedure is applied to the remaining clusters, with cluster 1 displayed here.}

  \label{fig:cluster3}
\end{figure}


\subsection{Merging Augmented Clusters}

After independently oversampling each cluster, the resulting augmented clusters are merged into a single balanced dataset, as illustrated in Figure~\ref{fig:merging}. Formally, each augmented cluster is defined as:
\[
\hat{\mathcal{D}}_k = \mathcal{D}_k \cup \mathcal{D}_k^{*},
\]
where \(\mathcal{D}_k^{*}\) represents the synthetic samples generated for cluster \(k\). The final augmented dataset \(\hat{\mathcal{D}}\) is constructed by merging all augmented clusters:
\[
\hat{\mathcal{D}} = \bigcup_{k=1}^{K}\hat{\mathcal{D}}_k.
\]
The size of each cluster after augmentation is explicitly given by:
\[
|\hat{\mathcal{D}}_k| = n_k' = \lceil \alpha_k n_k \rceil.
\]
Hence, the total size of the merged dataset becomes:
\[
|\hat{\mathcal{D}}| = \sum_{k=1}^K n_k' = \sum_{k=1}^K \left(n_k + (n_k' - n_k)\right).
\]
By carefully selecting the multipliers \(\alpha_k\), we precisely control the number of synthetic points contributed by each cluster. This yields an augmented representation across the entire range of frequent and rare target values. The complete LDAO procedure is presented in Algorithm~\ref{alg:ldao}, which outlines all four steps of our proposed method.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{merging.pdf}
  \caption{\footnotesize visualization of original (filled markers) and synthetic (outlined markers) points in each cluster after local oversampling. The merged dataset is augmented while preserving all original data.}
  \label{fig:merging}
\end{figure}


% \begin{algorithm}[H]
% \SetAlgoLined\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
% \caption{\textbf{Local Distribution-Based Adaptive Oversampling (LDAO)}}
% \label{alg:ldao}
% \Input{Dataset $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$, $\mathbf{x}_i \in \mathbb{R}^{d}$, $y_i \in \mathbb{R}$; Candidate cluster range: $K_{\min}$ to $K_{\max}$; Oversampling multipliers $\{\alpha_k\}_{k=1}^{K}$}
% \Output{Augmented dataset $\hat{\mathcal{D}}$ with improved coverage of sparse target regions}
% \textbf{Step 1: Clustering in Joint Feature–Target Space: } Construct joint vectors $\mathbf{z}_i = (\mathbf{x}_i, y_i) \in \mathbb{R}^{d+1}$ for all $i=1,\dots,N$. \For{$K = K_{\min}$ \KwTo $K_{\max}$}{ Run $k$-means clustering minimizing SSE:
% \[
% \underset{\{\boldsymbol{\mu}_k\}_{k=1}^K,\{c_i\}_{i=1}^N}{\arg\min}\;\sum_{i=1}^{N}\|\mathbf{z}_i - \boldsymbol{\mu}_{c_i}\|^2,\quad \mathrm{SSE}(K) = \sum_{k=1}^{K}\sum_{\mathbf{z}\in\mathcal{D}_k}\|\mathbf{z}-\boldsymbol{\mu}_k\|^2.
% \]
% } Compute relative SSE reduction:
% \[
% \Delta(K) = \frac{\mathrm{SSE}(K-1)-\mathrm{SSE}(K)}{\mathrm{SSE}(K-1)},\quad K=2,\dots,K_{\max}.
% \]
% Select optimal $K^*$ as the value where $\Delta(K)$ shows a significant drop-off (the `elbow point') and cluster data using optimal $K^*$, obtaining clusters $\{\mathcal{D}_k\}_{k=1}^{K^*}$.\\[1mm]
% \textbf{Step 2: Local Kernel Density Estimation: } \For{$k = 1$ \KwTo $K^*$}{ Fit Gaussian KDE on cluster $\mathcal{D}_k=\{\mathbf{z}_j^{(k)}\}_{j=1}^{n_k}$:
% \[
% \hat{f}_k(\mathbf{z}) = \frac{1}{n_k}\sum_{j=1}^{n_k}\frac{1}{(2\pi)^{\frac{d+1}{2}}|\mathbf{H}|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_j^{(k)})^{\top}\mathbf{H}^{-1}(\mathbf{z}-\mathbf{z}_j^{(k)})\right).
% \]
% }\\[1mm]
% \textbf{Step 3: Synthetic Data Generation: } \For{$k = 1$ \KwTo $K^*$}{ Compute augmented cluster size:
% \[
% n_k' = \lceil \alpha_k n_k \rceil,
% \]
% generate exactly $(n_k'-n_k)$ synthetic samples $\mathbf{z}^{*}$ from KDE:
% \[
% \mathbf{z}^{*} = \mathbf{z}_j^{(k)} + \mathbf{H}^{\frac{1}{2}}\boldsymbol{\epsilon},\quad \boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d+1}),\quad j\sim\text{Uniform}\{1,\dots,n_k\},
% \]
% and store generated points as $\mathcal{D}_k^{*}$.
% }\\[1mm]
% \textbf{Step 4: Merge Augmented Clusters: } Combine original and synthetic points:
% \[
% \hat{\mathcal{D}}_k = \mathcal{D}_k\cup\mathcal{D}_k^{*},\quad k=1,\dots,K^*,
% \]
% merge clusters into final augmented dataset:
% \[
% \hat{\mathcal{D}}=\bigcup_{k=1}^{K^*}\hat{\mathcal{D}}_k.
% \]
% \Return $\hat{\mathcal{D}}$
% \end{algorithm}

\begin{algorithm}[H]
\SetAlgoLined\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{\textbf{Local Distribution-Based Adaptive Oversampling (LDAO)}}
\label{alg:ldao}
\Input{Dataset $\mathcal{D}=\{(\mathbf{x}_i,y_i)\}_{i=1}^{N}$, $\mathbf{x}_i\in\mathbb{R}^{d}$, $y_i\in\mathbb{R}$; cluster range: $K_{\min}$ to $K_{\max}$; oversampling multipliers $\{\alpha_k\}_{k=1}^K$}
\Output{Augmented dataset $\hat{\mathcal{D}}$}
Construct joint vectors $\mathbf{z}_i=(\mathbf{x}_i,y_i)\in\mathbb{R}^{d+1}$, $i=1,\dots,N$. \For{$K=K_{\min}$ \KwTo $K_{\max}$}{ Run $k$-means: $\min_{\{\boldsymbol{\mu}_k\},\{c_i\}}\sum_{i=1}^{N}\|\mathbf{z}_i-\boldsymbol{\mu}_{c_i}\|^2$, compute $\mathrm{SSE}(K)=\sum_{k=1}^{K}\sum_{\mathbf{z}\in\mathcal{D}_k}\|\mathbf{z}-\boldsymbol{\mu}_k\|^2$. } Compute $\Delta(K)=\frac{\mathrm{SSE}(K-1)-\mathrm{SSE}(K)}{\mathrm{SSE}(K-1)}$, $K=2,\dots,K_{\max}$; choose $K^*$ where $\Delta(K)$ drops significantly; cluster data into $\{\mathcal{D}_k\}_{k=1}^{K^*}$. \For{$k=1$ \KwTo $K^*$}{ Fit Gaussian KDE on $\mathcal{D}_k=\{\mathbf{z}_j^{(k)}\}_{j=1}^{n_k}$: $\hat{f}_k(\mathbf{z})=\frac{1}{n_k}\sum_{j=1}^{n_k}\frac{1}{(2\pi)^{\frac{d+1}{2}}|\mathbf{H}|^{1/2}}\exp\Bigl(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_j^{(k)})^\top\mathbf{H}^{-1}(\mathbf{z}-\mathbf{z}_j^{(k)})\Bigr)$. } \For{$k=1$ \KwTo $K^*$}{ Set $n_k'=\lceil\alpha_kn_k\rceil$, generate $(n_k'-n_k)$ samples $\mathbf{z}^*=\mathbf{z}_j^{(k)}+\mathbf{H}^{1/2}\boldsymbol{\epsilon}$ with $\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d+1})$, $j\sim\text{Uniform}\{1,\dots,n_k\}$, store as $\mathcal{D}_k^*$. } Define $\hat{\mathcal{D}}_k=\mathcal{D}_k\cup\mathcal{D}_k^*$ for each $k$, and merge: $\hat{\mathcal{D}}=\bigcup_{k=1}^{K^*}\hat{\mathcal{D}}_k$. \Return $\hat{\mathcal{D}}$
\end{algorithm}




% ------------------------
%   Research methodology
% ------------------------
\section{Evaluation methodology}
\label{sec:researchmethodology}


We evaluated LDAO with current state-of-the-art imbalanced regression approaches using a diverse collection of benchmark datasets. This section details our experimental design, including dataset selection, baseline methods, implementation specifics, hyperparameter tuning, and validation procedures. 


\subsection{Datasets}

We evaluated our method using 45 datasets from three sources: the Keel repository \cite{alcala2011}, the collection at \url{https://paobranco.github.io/DataSets-IR} \cite{branco2019}, and the repository at \url{https://github.com/JusciAvelino/imbalancedRegression} \cite{avelino2024}. 



% Define a clean color scheme
\definecolor{headercolor}{RGB}{70, 130, 180}
\definecolor{rowcolor}{RGB}{240, 248, 255}



\vspace{0.5cm}
\begin{table}[H]
\centering
\small % Reduce font size
\setlength{\tabcolsep}{4pt} % Reduce column spacing
\caption{\footnotesize Dataset characteristics showing number of instances, features, and computed size category for various datasets.}
\label{tab:dataset-characteristics}
\resizebox{0.7\textwidth}{!}{ % Scale to fit text width
\begin{tabular}{lrrl|lrrl}
\toprule
\textbf{Dataset} & \textbf{Instances} & \textbf{Features} & \textbf{Size} & \textbf{Dataset} & \textbf{Instances} & \textbf{Features} & \textbf{Size} \\
\midrule
A1              & 198    & 11 & Small  & DEBUTANIZER          & 2,394  & 7  & Large  \\
A2              & 198    & 11 & Small  & DEE                  & 365    & 6  & Small  \\
A3              & 198    & 11 & Small  & DIABETES             & 43     & 2  & Small  \\
A7              & 198    & 11 & Small  & ELE-1                & 495    & 2  & Small  \\
ABALONE         & 4,177  & 8  & Large  & ELE-2                & 1,056  & 4  & Medium \\
ACCELERATION    & 1,732  & 14 & Medium & FORESTFIRES          & 517    & 12 & Medium \\
AIRFOILD        & 1,503  & 5  & Medium & FRIEDMAN             & 1,200  & 5  & Medium \\
ANALCAT         & 450    & 11 & Small  & FUEL                 & 1,764  & 37 & Medium \\
AUTOMPG6        & 392    & 5  & Small  & HEAT                 & 7,400  & 11 & Large  \\
AUTOMPG8        & 392    & 7  & Small  & HOUSE                & 22,784 & 16 & Large  \\
AVAILABLE\_POWER& 1,802  & 15 & Medium & KDD                  & 316    & 18 & Small  \\
BASEBALL        & 337    & 16 & Small  & LASER                & 993    & 4  & Medium \\
BOSTON          & 506    & 13 & Medium & LUNGCANCER           & 442    & 24 & Small  \\
CALIFORNIA      & 20,640 & 8  & Large  & MACHINECPU           & 209    & 6  & Small  \\
COCOMO          & 60     & 56 & Small  & CONCRETE\_STRENGTH   & 1,030  & 8  & Medium \\
COMPACTIV       & 8,192  & 21 & Large  & META                 & 528    & 65 & Medium \\
MORTGAGE        & 1,049  & 15 & Medium & MAXIMAL\_TORQUE      & 1,802  & 32 & Medium \\
PLASTIC         & 1,650  & 2  & Medium & CPU                  & 8,192  & 12 & Large  \\
POLE            & 14,998 & 26 & Large  & TRIAZINES            & 186    & 60 & Small  \\
QUAKE           & 2,178  & 3  & Large  & WANKARA              & 1,609  & 9  & Medium \\
SENSORY         & 576    & 11 & Medium & WINE\_QUALITY        & 1,143  & 12 & Medium \\
STOCK           & 950    & 9  & Medium & WIZMIR               & 1,461  & 9  & Medium \\
TREASURY        & 1,049  & 15 & Medium &                      &        &    &        \\
\bottomrule
\end{tabular}
}
\end{table}


These datasets span multiple domains and vary in size, dimensionality, and degree of imbalance, making them standard benchmarks for the rigorous evaluation of imbalanced regression methods and enabling fair comparisons with existing approaches. Table~\ref{tab:dataset-characteristics} shows the number of instances and features for each dataset.~For further analysis in the results section, we categorized the datasets based on their number of instances. Specifically, datasets with fewer than 500 instances are labeled as "Small", those with between 500 and 1999 instances as "Medium", and those with more than 1999 instances as "Large". This categorization facilitates a detailed evaluation of our method’s performance relative to dataset size.





\subsection{Metrics}
We evaluate LDAO against state-of-the-art approaches using multiple metrics that measure performance on both frequent and rare target values.

\subsubsection{Root Mean Square Error (RMSE)}
Root Mean Square Error (RMSE) measures the overall prediction accuracy by calculating 
the square root of the average squared difference between predicted values and actual observations:
\begin{equation}
   \text{RMSE} 
   = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2},
\end{equation}
where \(y_i\) represents the true target value and \(\hat{y}_i\) the predicted value 
for the \(i\)-th instance. RMSE provides a general assessment of model performance 
but can be dominated by errors in densely populated regions.

\subsubsection{Squared Error-Relevance Area (SERA)}
This metric provides a flexible way to evaluate models under non-uniform domain preferences. 
Let \(\phi(\cdot)\colon \mathcal{Y} \to [0,1]\) be a relevance function that assigns higher 
scores to more important (for example, rare or extreme) target values. Then for any relevance 
threshold \(t\), let
\[
D^t 
= \{(x_i,y_i)\mid \phi(y_i)\ge t\},
\]
and define
\begin{equation}
  SER_t 
  = \sum_{(x_i,y_i)\in D^t} \bigl(\hat{y}_i - y_i\bigr)^{2}.
\end{equation}

SERA then integrates this quantity over all \(t\in[0,1]\):
\begin{equation}
  SERA
  = \int_{0}^{1} SER_t \,dt
  = \int_{0}^{1} \sum_{(x_i,y_i)\in D^t} \bigl(\hat{y}_i - y_i\bigr)^{2}\,dt.
\end{equation}

SERA weights prediction errors by \(\phi(y_i)\), emphasizing performance on extreme values while still considering accuracy across the entire domain. This makes it well-suited for imbalanced regression, where predicting rare values accurately is crucial \cite{ribeiro2020}.

\subsubsection{Mean Absolute Error (MAE)}
Mean Absolute Error (MAE) is an alternative error metric that evaluates the overall prediction performance by averaging the absolute differences between the predicted and actual values:
\begin{equation}
   \text{MAE} 
   = \frac{1}{n}\sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|.
\end{equation}
MAE is less sensitive to outliers compared to RMSE and provides a straightforward interpretation of the average prediction error.

\subsection{Machine Learning Algorithms}
Following the approach of \cite{steininger2021}, we evaluated all methods using a Multi-Layer Perceptron (MLP) with three hidden layers (10 neurons each) and ReLU activations. The output layer uses linear activation for regression. We trained models for 1000 epochs using Adam optimizer with early stopping to prevent overfitting. We compared LDAO against four approaches: Baseline (no resampling, using the original imbalanced data), SMOGN (an extension of SMOTER that incorporates Gaussian noise during oversampling), G-SMOTE (Geometric SMOTE adapted for regression tasks, using geometric interpolation), and DenseLoss (a cost-sensitive approach that weights errors by target density). These methods represent the current state-of-the-art method for handling imbalanced regression.




\subsection{Implementation Resources}
For our evaluation metrics, we utilized the SERA implementation from the ImbalancedLearningRegression Python package \cite{wu2022}.~The SMOGN method was implemented using the package developed by Kunz \cite{kunz2020}. We implemented the DenseLoss and G-SMOTE methods based on their original papers, carefully following the authors' descriptions and guidelines to ensure faithful reproduction of their approaches. To optimize the hyperparameters for each method, we employed the Optuna framework \cite{akiba2019}. Optuna leverages Bayesian optimization using the Tree-structured Parzen Estimator (TPE) sampler, which efficiently balances exploration and exploitation during the search process. By tuning hyperparameters separately for each dataset, our approach accommodates the unique statistical properties and distribution characteristics inherent to each dataset, ensuring optimal performance for all methods.

\subsection{Experimental Framework}
We employed 5 runs of 5-fold cross-validation for all experiments. This outer fold cross-validation divided each dataset into five equal portions, with each fold using four portions (80\% of data) for training and one portion (20\% of data) as the test set. Each data portion served as a test set exactly once across the five folds in each run. For hyperparameter tuning within each fold, we further divided the training data into sub-training (80\%) and validation (20\%) sets. We utilized Bayesian optimization with 15 trials to efficiently search the parameter space, as it generally finds better hyperparameter values than a grid search while requiring fewer evaluations.


Table \ref{tab:hyperparameters} presents the hyperparameters and search ranges for each method. LDAO's parameters include the oversampling multiplier and KDE bandwidth. SMOGN uses neighborhood size, sampling approach, and relevance threshold. G-SMOTE involves quantile for rarity, truncation factor, deformation factor, number of neighbors, and oversampling factor. DenseLoss works with the density weighting parameter.
\begin{table}[H]
\centering
\caption{\footnotesize Hyperparameter search spaces for each compared method}
\label{tab:hyperparameters}
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{llll}
\toprule
\textbf{Method} & \textbf{Hyperparameter} & \textbf{Range} & \textbf{Description} \\
\midrule
LDAO  & K (candidate clusters) & [2, 6] & Number of clusters tested (optimal K is selected using the elbow method) \\
      & Multiplier per cluster & [1.0, 3.0] & Oversampling factor \\
      & Bandwidth per cluster  & [0.1, 2.0] & KDE smoothing parameter \\
\midrule
SMOGN & k                   & \{3, 5, 7, 9\} & Number of neighbors \\
      & sampling\_method     & \{extreme, balance\} & Sampling approach \\
      & rel\_thres           & [0.0, 1.0] & Relevance threshold \\
\midrule
G-SMOTE & q\_rare          & [0.05, 0.25] & Quantile for rarity \\
        & truncation\_factor & [0.0, 0.9]   & Geometric truncation \\
        & deformation\_factor & [0.0, 0.9]  & Geometric deformation \\
        & k\_neighbors      & [2, 5]       & Number of neighbors \\
        & oversampling\_factor & [1.0, 5.0] & Amount of oversampling \\
\midrule
DenseLoss & alpha           & [0.0, 2.0]   & Density weighting factor \\
\bottomrule
\end{tabular}%
}
\end{table}







% ------------------------
% Results Section
% ------------------------
\section{Results}
\label{sec:results}



\noindent
We present the results of comparing LDAO with state-of-the-art oversampling methods across various performance dimensions. In Figure \ref{fig:four_figures}, we conduct pairwise comparisons for each evaluation metric (RMSE, SERA, MAE) to demonstrate how LDAO performs relative to each competing method individually. RMSE and MAE evaluate overall model performance across the entire target distribution, while SERA places greater emphasis on errors associated with rare samples.

\smallskip

For each dataset, a winner is determined based on the outcomes over 25 folds, meaning that each dataset contributes one win, and there are 45 wins in total. Specifically, within each dataset, the method that prevails in the majority of the 25 folds is declared the winner. This procedure is repeated against each competing method. Next, we perform the Wilcoxon Signed-Rank Test to compare LDAO’s performance vector with that of the competing method. In this test, we compute the differences between the 25-fold loss metric values of LDAO and those of the competing method to construct a difference vector. The Wilcoxon test then examines whether these differences are statistically significant, using an alpha level of 0.05, which indicates a 5\% risk of incorrectly concluding that a difference exists when there is none \cite{wilcoxon1945}.

\smallskip

In Figure \ref{fig:four_figures}, the lighter color represents the total number of wins for each method, while the darker portion indicates the number of wins that are statistically significant. The figure illustrates that LDAO outperforms all competing methods across all evaluated metrics. Although SMOGN exhibits lower performance in terms of RMSE and MAE, its SERA measurement shows significantly superior performance in more tests, aligning with its emphasis on rare samples; nevertheless, SMOGN remains inferior to LDAO overall. The other methods display a more consistent performance across the different metrics.






% Define colors for the legend
\definecolor{ldaocolor}{RGB}{59, 130, 246}      % Blue
\definecolor{baselinecolor}{RGB}{239, 68, 68}   % Red
\definecolor{smogncolor}{RGB}{34, 197, 94}      % Green
\definecolor{denselosscolor}{RGB}{168, 85, 247} % Purple
\definecolor{gsmotecolor}{RGB}{250, 204, 21}    % Yellow
\definecolor{verydarkblue}{RGB}{0,0.1,0.4}      % Dark blue for winners

% Define winner command for tables
\newcommand{\winner}[1]{\textbf{\textcolor{verydarkblue}{#1}}}


\begin{figure}[H]
    \centering
    % First row with two subfigures of 0.33 width each
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{LDAO_SMOGN.pdf}
        %\caption{\footnotesize LDAO and SMOGN.}
        \label{fig:LDAO_SMOGN}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{LDAO_BASELINE.pdf}
        %\caption{\footnotesize LDAO and BASELINE.}
        \label{fig:LDAO_BASELINE}
    \end{subfigure}

    % Minimal vertical space between rows.
    \vspace{2mm}

    % Second row with two subfigures of 0.33 width each
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{LDAO_GSMOTE.pdf}
        %\caption{\footnotesize LDAO and G-SMOTE.}
        \label{fig:LDAO_GSMOTE}
    \end{subfigure}%
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{LDAO_DENSELOSS.pdf}
        %\caption{\footnotesize LDAO and DENSELOSS.}
        \label{fig:LDAO_DENSELOSS}
    \end{subfigure}

    \caption{\footnotesize Pairwise comparisons of LDAO with SMOGN, Baseline, G-SMOTE, and DenseLoss across RMSE, SERA, and MAE. Light-colored bars indicate total wins, while darker segments represent statistically significant wins (\(\alpha=0.05\)).}
    \label{fig:four_figures}
\end{figure}


In Figure \ref{fig:combined_scores}, we computed both the median and the mean of various metrics across all datasets for each method. In this analysis, the methods were evaluated collectively rather than in pairwise comparisons. The blue circle represents the baseline benchmark, and the percentages, whether negative or positive, indicate how each oversampling technique performs relative to this baseline. 

\smallskip

LDAO again demonstrates robust performance and surpasses its peer methods even when all techniques are assessed together. In each figure, the top three methods are highlighted, including the baseline. The only circumstance in which a peer method, SMOGN, outperformed LDAO occurred when calculating the mean SERA value; in that specific case, SMOGN achieved slightly better results, although both methods significantly outperformed the others. For the SERA mean computation across datasets, a base-10 logarithmic scale was employed to accommodate the high loss values observed on rare samples in some datasets, thereby enhancing clarity.


\begin{figure}[H]
  \centering
  % First subfigure for median scores
  \begin{subfigure}[b]{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{median_metric.pdf}
    \label{fig:median_sub}
  \end{subfigure}
  
  % \vspace{1em} % Adjust vertical space as needed
  
  % Second subfigure for mean scores
  \begin{subfigure}[b]{0.8\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mean_metric.pdf}
    \label{fig:mean_sub}
  \end{subfigure}
  
  \caption{\footnotesize Performance evaluation of resampling methods across datasets. The top panel shows the median scores, while the bottom panel presents the mean scores.}
  \label{fig:combined_scores}
\end{figure}



Table~\ref{tab:rank_summary_compact} presents the mean rank of each method for RMSE, SERA, and MAE across all 45 datasets. A Rank is the position given to a method when methods are sorted by performance for a given dataset and metric. Rank 1 for the lowest error (best) and Rank 5 for the highest error (worst). For each dataset and metric, a method’s rank was first averaged over 25 cross‑validation folds; these per‑dataset ranks were then averaged across all datasets to produce the Mean column. The R1 column indicates the number of datasets in which a method achieved the best (lowest) rank, and the R5 column indicates the number of datasets in which it achieved the worst (highest) rank. In cases where two or more methods shared virtually identical best or worst values, no count was assigned, so some column totals may sum to less than 45. 

\smallskip

As shown in Table~\ref{tab:rank_summary_compact}, LDAO outperforms the other methods, achieving the lowest mean ranks and the highest R1 counts across RMSE, SERA, and MAE, while SMOGN exhibits the highest R5 counts for RMSE and MAE, indicating the weakest performance on those metrics. Baseline, DenseLoss, and G‑SMOTE show intermediate performance, with G‑SMOTE also recording a high R5 count in SERA.









\smallskip


Figure \ref{fig:ranking-combined} offers an alternative view by displaying the mean ranking of each method as a function of dataset size. The rankings separate the data sets into small, medium and large based on the classifications previously given in table~\ref{tab:dataset-characteristics}. This evaluation aims to determine how oversampling methods perform across different dataset sizes, as some methods might be more effective on smaller datasets while others excel with larger ones.

\smallskip

LDAO consistently outperformed competing methods, achieving the lowest mean ranking across all metrics and dataset sizes. The only case in which a competing method performed slightly better was for the SERA metric on large datasets, where SMOGN outperformed LDAO; however, LDAO remained superior for both small and medium datasets. In this figure, the blue circle represents the baseline model benchmark, providing a reference for the performance of the various methods. 


% in your document preamble:
\definecolor{darkgreen}{RGB}{0,100,0}
\definecolor{darkred}{RGB}{139,0,0}

% then your table:
\begin{table}[htbp]
  \centering
  \scriptsize
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{\footnotesize Mean rank and counts of best (Rank\,1) and worst (Rank\,5) across 45 datasets. The lowest mean ranks are highlighted in bold black, the highest Rank\,1 counts in bold green, and the highest Rank\,5 counts in bold red.}

  \label{tab:rank_summary_compact}
  \begin{tabular}{@{}l
                   rrr
                   rrr
                   rrr@{}}
    \toprule
     & \multicolumn{3}{c}{RMSE}
     & \multicolumn{3}{c}{SERA}
     & \multicolumn{3}{c}{MAE} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
    Method   & Mean & R1  & R5
              & Mean & R1  & R5
              & Mean & R1  & R5 \\
    \midrule
    \textbf{LDAO}
              & \textbf{2.151}
               & \textcolor{darkgreen}{\textbf{27}}
               & 4
              & \textbf{2.491}
               & \textcolor{darkgreen}{\textbf{25}}
               & 7
              & \textbf{2.001}
               & \textcolor{darkgreen}{\textbf{29}}
               & 2 \\
    BASELINE  & 2.817          & 9           & 1
              & 3.106          & 2           & 5
              & 2.843          & 6           & 1 \\
    DENSELOSS & 3.257          & 3           & 12
              & 3.118          & 4           & 10
              & 3.353          & 0           & 11 \\
    G‑SMOTE   & 3.025          & 5           & 5
              & 3.286          & 1           & \textcolor{darkred}{\textbf{14}}
              & 2.867          & 7           & 2 \\
    SMOGN     & 3.749          & 1           & \textcolor{darkred}{\textbf{22}}
              & 2.998          & 10          & 9
              & 3.936          & 1           & \textcolor{darkred}{\textbf{28}} \\
    \bottomrule
  \end{tabular}
\end{table}




\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.75\textwidth}
    \includegraphics[width=\textwidth]{small_dataset_rank_visual.pdf}
  \end{minipage}\\[1ex]
  \begin{minipage}[t]{0.75\textwidth}
    \includegraphics[width=\textwidth]{medium_dataset_rank_visual.pdf}
  \end{minipage}\\[1ex]
  \begin{minipage}[t]{0.75\textwidth}
    \includegraphics[width=\textwidth]{large_dataset_rank_visual.pdf}
  \end{minipage}
  \caption{\footnotesize Mean ranking of oversampling methods by dataset size category. The top, middle, and bottom panels correspond to small, medium, and large datasets as defined in Table~\ref{tab:dataset-characteristics}.}
  \label{fig:ranking-combined}
\end{figure}




\section{Discussion}
\label{sec:conclusion}

An interesting observation is the strong performance of the baseline model relative to the oversampling methods. This result suggests that oversampling techniques may underperform without meticulous fine-tuning informed by domain knowledge and underscores the inherent complexity of the imbalanced regression problem compared to classification. In many continuous oversampling approaches, samples are categorized as either rare or frequent; although such a biased assumption can sometimes assist the learning process, it may also lead to diminished performance compared to the baseline. A key attribute of LDAO is its completely data-driven nature. Despite having a few parameters to adjust, LDAO does not require extensive domain expertise, making it readily applicable to a wide range of imbalanced datasets.

\smallskip

Oversampling approaches have the benefit of increasing representation in sparse regions. By generating synthetic samples, these methods can help models learn better from rare target values and improve performance where data is limited. The process involves creating additional data points in underrepresented areas, allowing machine learning models to develop a more complete understanding of the entire target value range. For regression tasks with imbalanced distributions, these techniques provide a practical solution to improve prediction accuracy across the entire spectrum of outcomes.

\smallskip

Specifically for LDAO, the approach leverages local clustering and density estimation to generate synthetic samples that more closely match the underlying data structure. LDAO aims to enhance model performance in regions where simpler approaches might struggle due to data scarcity. The method's focus on local density awareness makes it particularly useful for complex datasets with varying degrees of sparsity. LDAO especially examine sparsity in the joint distribution of features ($X$) and target values ($y$), providing a more detailed and thorough understanding of where data is truly scarce in the multidimensional space.

\smallskip

Despite these advantages, oversampling methods encounter several limitations. They risk overfitting when the synthetic points are too similar to existing samples and may introduce noise if they fail to accurately capture the underlying distribution. Moreover, these techniques typically require additional parameter tuning, such as selecting appropriate rarity thresholds and determining suitable oversampling factors. In LDAO's case, additional considerations include choosing the clustering parameters, specifying the number of synthetic samples, and configuring the density estimation settings. Without proper calibration, synthetic samples might overfit local patterns or miss critical variations, thereby diminishing the overall effectiveness of the approach when the underlying data characteristics or sparsity patterns are not well captured by the chosen method.



















% ------------------------
% Conclusion
% ------------------------
\section{Conclusion}
\label{sec:conclusion}


In this work, we proposed LDAO, a local distribution-based adaptive oversampling method specifically designed to address the challenges of imbalanced regression. By modeling data in a joint feature–target space and generating synthetic samples independently within identified clusters, LDAO effectively preserves the original dataset's statistical structure. Through comprehensive evaluation on a wide variety of datasets, we show that LDAO achieves strong and consistent predictive accuracy, frequently surpassing leading data‑level and algorithm‑level approaches, particularly where data are sparse.

\smallskip

Oversampling methods, in general, have proven effective at handling imbalance by improving model representation in sparse areas, yet each method has its unique strengths and applications. LDAO contributes to this field by eliminating the need for predefined rarity thresholds and undersampling, thereby preserving valuable information and offering adaptive, data-driven augmentation. As shown in our experiments, these attributes enable LDAO to maintain overall predictive accuracy while enhancing performance in challenging, underrepresented regions. Future work should continue to refine adaptive density-based sampling methods, particularly for datasets with complex, multimodal distributions.


\section*{Declarations}

\subsection*{Competing Interests}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.


% ------------------------
%     REFERENCES
% ------------------------
\begin{thebibliography}{99}

% Reordered according to appearance in text (numeric labels).
% ----------------------------------------------------------

%1
\bibitem{he2009}
H.~He and E.~A.~Garcia, “Learning from imbalanced data,” \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~21, no.~9, pp. 1263--1284, 2009.
%2
\bibitem{haixiang2017} H.~Guo, Y.~Li, J.~Shang, M.~Gu, Y.~Huang, and B.~Gong, “Learning from class-imbalanced data: Review of methods and applications,” \emph{Expert Systems with Applications}, vol.~73, pp. 220--239, 2017.
%3
\bibitem{chawla2002}
N.~V. Chawla, K.~W. Bowyer, L.~O. Hall, and W.~P. Kegelmeyer,
“SMOTE: Synthetic minority over-sampling technique,”
\emph{Journal of Artificial Intelligence Research}, vol.~16, pp. 321--357, 2002.
%4
\bibitem{buda2018}
M.~Buda, A.~Maki, and M.~A.~Mazurowski, “A systematic study of the class imbalance problem in convolutional neural networks,” 
\emph{Neural Networks}, vol.~106, pp. 249--259, 2018.
%5
\bibitem{johnson2019}
J.~M.~Johnson and T.~M.~Khoshgoftaar, “Survey on deep learning with class imbalance,”
\emph{Journal of Big Data}, vol.~6, no.~1, pp. 1--54, 2019.
%6
\bibitem{liu2009}
X.-Y.~Liu, J.~Wu, and Z.-H.~Zhou, “Exploratory undersampling for class-imbalance learning,”
\emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, vol.~39, no.~2, pp. 539--550, 2009.
%7
\bibitem{krawczyk2016}
B.~Krawczyk, “Learning from imbalanced data: open challenges and future directions,”
\emph{Progress in Artificial Intelligence}, vol.~5, no.~4, pp. 221--232, 2016.
%8
\bibitem{branco2016}
P.~Branco, L.~Torgo, and R.~P.~Ribeiro, “A survey of predictive modeling under imbalanced distributions,”
\emph{ACM Computing Surveys}, vol.~49, no.~2, Article 31, 2016.
%9
\bibitem{torgo2013}
L.~Torgo, R.~P. Ribeiro, J.~P. da Costa, and S.~Pal,
“SMOTE for regression,” in
\emph{Intelligent Data Engineering and Automated Learning (IDEAL 2013). Lecture Notes in Computer Science}, vol.~8206, 2013, pp. 378--387.
%10
\bibitem{chawla2004}
N.~V.~Chawla, N.~Japkowicz, and A.~Kolcz, “Editorial: Special issue on learning from imbalanced data sets,”
\emph{ACM SIGKDD Explorations Newsletter}, vol.~6, no.~1, pp. 1--6, 2004.
%11
\bibitem{kaur24}
A.~Kaur and M.~Sarmadi, “Comparative analysis of machine learning techniques
for imbalanced genetic data,” \emph{Annals of Data Science}, 2024.
%12
\bibitem{scheepens23}
D.~Scheepens, I.~Schicker, K.~Hlaváčková-Schindler, and C.~Plant,
“Adapting a deep convolutional RNN model with imbalanced regression loss
for improved spatio-temporal forecasting of extreme wind speed events 
in the short to medium range,” \emph{Geosci. Model Dev.}, 2023.
%13
\bibitem{xu2022}
Z.~Xu, C.~Zhao, C.~D.~Scales~Jr, R.~Henao, and B.~A.~Goldstein,
“Predicting in-hospital length of stay: a two-stage modeling approach to account for highly skewed data,” 
\emph{BMC Medical Informatics and Decision Making}, vol.~22, article~110, 2022.
%14
\bibitem{branco2017}
P.~Branco, L.~Torgo, and R.~P. Ribeiro,
“SMOGN: A pre-processing approach for imbalanced regression,” in
\emph{Proceedings of Machine Learning Research: LIDTA}, vol.~74, 2017, pp. 36--50.
%15
\bibitem{steininger2021}
M.~Steininger, K.~Kobs, P.~Davidson, A.~Krause, and A.~Hotho, 
“Density-based weighting for imbalanced regression,”
\emph{Machine Learning}, vol. 110, no. 8, pp. 2187--2210, 2021.
%16
\bibitem{he2008}
H.~He, Y.~Bai, E.~A. Garcia, and S.~Li, “ADASYN: Adaptive synthetic sampling approach for imbalanced learning,” in
\emph{Proceedings of the International Joint Conference on Neural Networks}, 2008, pp. 1322--1328.
%17
\bibitem{han2005}
H.~Han, W.-Y.~Wang, and B.-H.~Mao, “Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning,” in
\emph{Proceedings of ICIC 2005}, 2005, pp. 878--887, Springer.
%18
\bibitem{bunkhumpornpat2012}
C.~Bunkhumpornpat, K.~Sinapiromsaran, and C.~Lursinsap, “DBSMOTE: Density-based synthetic minority over-sampling technique,” \emph{Applied Intelligence}, vol.~36, no.~3, pp.~664--684, 2012.
%19
\bibitem{he2013}
H.~He and Y.~Ma,
\emph{Imbalanced learning: Foundations, algorithms, and applications}.
John Wiley \& Sons, 2013.
%20
\bibitem{torgo2007}
L.~Torgo and R.~P.~Ribeiro, “Utility-based regression,” in
\emph{Proceedings of PKDD 2007}, 2007, pp.~597--604, Springer.
%21
\bibitem{ribeiro2011a}
R.~P.~A.~Ribeiro, \emph{Utility-based regression} (Ph.D. thesis), Porto: Faculty of Sciences, University of Porto, 2011.
%22
\bibitem{torgo2015}
L.~Torgo, P.~Branco, R.~P.~Ribeiro, and B.~Pfahringer, “Resampling strategies for regression,” \emph{Expert Systems}, vol.~32, no.~3, pp.~465--476, 2015.
%23
\bibitem{branco2019}
P.~Branco, L.~Torgo, and R.~P.~Ribeiro, “Pre-processing approaches for imbalanced distributions in regression,”
\emph{Neurocomputing}, vol.~343, pp.~76--99, 2019.
%24
\bibitem{camacho2022}
L.~Camacho, G.~Douzas, and F.~Bacao,
“Geometric SMOTE for regression,”
\emph{Expert Systems with Applications}, vol. 193, 116387, 2022.
%25
\bibitem{stocksieker2023}
S.~Stocksieker, D.~Pommeret, and A.~Charpentier, “Data Augmentation for Imbalanced Regression,” in \emph{Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS)}, 2023.
%26
\bibitem{camacho2024}
L.~Camacho and F.~Bacao, “WSMOTER: A Novel Approach for Imbalanced Regression,” \emph{Applied Intelligence}, vol.~54, pp. 8789--8799, 2024.
%27
\bibitem{stocksieker2024}
S.~Stocksieker, D.~Pommeret, and A.~Charpentier, “Generalized Oversampling for Learning from Imbalanced Datasets and Associated Theory: Application in Regression,” \emph{Transactions on Machine Learning Research}, vol.~6, 2024.
%28
\bibitem{aleksic2025}
J.~Aleksic and M.~Garc{\'i}a-Remesal, “A Selective Under-Sampling (SUS) Method for Imbalanced Regression,” \emph{Journal of Artificial Intelligence Research}, vol.~82, pp. 111--136, 2025.
%29
\bibitem{zhou2010}
Z.-H.~Zhou and X.-Y.~Liu, “On multi-class cost-sensitive learning,”
\emph{Computational Intelligence}, vol.~26, no.~3, pp.~232--257, 2010.
%30
\bibitem{elkan2001}
C.~Elkan, “The foundations of cost-sensitive learning,” in
\emph{Proceedings of the 17th International Joint Conference on Artificial
Intelligence (IJCAI)}, 2001, pp. 973--978.
%31
\bibitem{domingos1999}
P.~Domingos, “MetaCost: A general method for making classifiers
cost-sensitive,” in \emph{Proceedings of the 5th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD)}, 1999, pp. 155--164.
%32
\bibitem{yang2021} J. Yang, L. Xie, Q. Yu, X. He, and J. Liu, “Delving into deep imbalanced regression,” in \emph{Proceedings of the 38th International Conference on Machine Learning (ICML)}, pp. 8437-8447, 2021.
%33
\bibitem{ren2022} M. Ren, W. Luo, and R. Urtasun, “Balanced MSE for imbalanced visual regression,” in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 418-427, 2022.
%34
\bibitem{hoens2013}
T.~R.~Hoens and N.~V.~Chawla, “Imbalanced datasets: From sampling to classifiers,” in
\emph{Imbalanced Learning: Foundations, Algorithms, and Applications}, Wiley, 2013, pp.~43--59.
%35
\bibitem{moniz2018} N. Moniz, L. Torgo, and C. Soares, “SMOTEBoost for regression: Improving the prediction of extreme values,” in \emph{Proceedings of the 5th International Conference on Data Science and Advanced Analytics (DSAA)}, pp. 127-136, 2018.
%36
\bibitem{ribeiro2020}
R.~P.~Ribeiro and N.~Moniz, “Imbalanced regression and extreme value prediction,”
\emph{Machine Learning}, vol.~109, no.~9--10, pp.~1803--1835, 2020.
%37
\bibitem{wang2020} X. Wang, J. Xu, T. Zeng, and L. Jing, "Local Distribution-based Adaptive Minority Oversampling for Imbalanced Data Classification," \emph{Neurocomputing}, 2020, doi:10.1016/j.neucom.2020.05.030.
%38
\bibitem{harrison1978hedonic}
D.~Harrison and D.~L.~Rubinfeld, "Hedonic Prices and the Demand for Clean Air," \emph{Journal of Environmental Economics and Management}, vol.~5, no.~1, pp.~81--102, 1978.
%39
\bibitem{sugar2003}
J.~Sugar and P.~James, "Finding the Number of Clusters in a Dataset: An Information-Theoretic Approach," \emph{Journal of the American Statistical Association}, vol.~98, no.~463, pp.~750--763, 2003.
%40
\bibitem{ikotun2023}
A.~Ikotun, A.~Ezugwu, L.~Abualigah, B.~Abuhaija, and H.~Jia, “K-means clustering algorithms: a comprehensive review, variants analysis, and advances in the era of big data,” \emph{Information Sciences}, vol.~622, pp.~178--210, 2023, doi:10.1016/j.ins.2022.11.139.
%41
\bibitem{syakur2018}
M.~Syakur, B.~Khotimah, E.~Rochman, and B.~Satoto, “Integration k-means clustering method and elbow method for identification of the best customer profile cluster,” \emph{IOP Conference Series: Materials Science and Engineering}, vol.~336, art.~012017, 2018, doi:10.1088/1757-899x/336/1/012017.
%42
\bibitem{nainggolan2019}
R.~Nainggolan, R.~Perangin-angin, E.~Simarmata, and A.~Tarigan, “Improved the performance of the k-means cluster using the sum of squared error (sse) optimized by using the elbow method,” \emph{Journal of Physics: Conference Series}, vol.~1361, no.~1, art.~012015, 2019, doi:10.1088/1742-6596/1361/1/012015.
%43
\bibitem{silverman1986}
B.~W.~Silverman, "Density Estimation for Statistics and Data Analysis," \emph{Monographs on Statistics and Applied Probability}, Chapman and Hall, London, 1986.
%44
\bibitem{scott2015}
D.~W.~Scott, "Multivariate Density Estimation: Theory, Practice, and Visualization," \emph{Wiley Series in Probability and Statistics}, 2nd ed., John Wiley \& Sons, Hoboken, New Jersey, 2015.
%45
\bibitem{sheather1991}
S.~J.~Sheather and M.~C.~Jones, "A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation," \emph{Journal of the Royal Statistical Society. Series B (Methodological)}, vol.~53, no.~3, pp.~683-690, 1991.
%46
\bibitem{zhang2006}
X.~Zhang, M.~L.~King, and R.~J.~Hyndman, "A Bayesian approach to bandwidth selection for multivariate kernel density estimation," \emph{Computational Statistics \& Data Analysis}, vol.~50, no.~11, pp.~3009-3031, 2006.
%47
\bibitem{alcala2011}
J.~Alcalá-Fdez, A.~Fernandez, J.~Luengo, J.~Derrac, S.~García, L.~Sánchez, and F.~Herrera, “KEEL Data-Mining Software Tool: Data Set Repository, Integration of Algorithms and Experimental Analysis Framework,” \emph{Journal of Multiple-Valued Logic and Soft Computing}, vol.~17, no.~2--3, pp.~255--287, 2011.
%48
\bibitem{avelino2024}
J.~G.~Avelino, G.~D.~C.~Cavalcanti, and R.~M.~O.~Cruz, “Resampling strategies for imbalanced regression: a survey and empirical analysis,” \emph{Artificial Intelligence Review}, vol.~57, art.~82, 2024.
%49
\bibitem{wu2022}
W.~Wu, N.~Kunz, and P.~Branco, "ImbalancedLearningRegression-A Python Package to Tackle the Imbalanced Regression Problem," in \emph{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, pp.~645--648, 2022.
%50
\bibitem{kunz2020}
N.~Kunz, "SMOGN: Synthetic Minority Over-Sampling Technique for Regression with Gaussian Noise," \emph{PyPI}, version v0.1.2, 2020.
%51
\bibitem{akiba2019}
T.~Akiba, S.~Sano, T.~Yanase, T.~Ohta, and M.~Koyama, "Optuna: A Next-Generation Hyperparameter Optimization Framework," in \emph{The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pp.~2623--2631, 2019.
%52
\bibitem{wilcoxon1945}
F.~Wilcoxon, "Individual comparisons by ranking methods," \emph{Biometrics Bulletin}, vol.~1, no.~6, pp.~80--83, 1945.


\end{thebibliography}

\end{document}


